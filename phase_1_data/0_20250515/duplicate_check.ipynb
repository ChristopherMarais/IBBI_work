{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be766c89-2959-498e-9dfb-57b213011e4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import hashlib\n",
    "# from PIL import Image\n",
    "# import imagehash # For aHash, pHash, dHash\n",
    "# import pandas as pd\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from collections import defaultdict\n",
    "# import time\n",
    "# import shutil # For robust saving\n",
    "\n",
    "# # from tqdm import tqdm # Optional: for a nicer progress bar\n",
    "\n",
    "# Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "# # --- Configuration ---\n",
    "# DEFAULT_IMAGE_FOLDER = r'/blue/hulcr/gmarais/PhD/phase_1_data/20250515/output' # User's default\n",
    "# OUTPUT_CSV_FILE = 'image_hashes_and_duplicates.csv'\n",
    "# MAX_WORKERS = os.cpu_count()  # Use all available CPU cores\n",
    "# SAVE_INTERVAL = 1000 # Number of NEWLY PROCESSED images before an interim save\n",
    "\n",
    "# # --- Hashing Algorithms ---\n",
    "# # Dhash and sha25 seem to work the best\n",
    "# HASH_ALGORITHMS = {\n",
    "#     'sha256': lambda img: hashlib.sha256(img.tobytes()).hexdigest(),\n",
    "#     # 'ahash': lambda img: str(imagehash.average_hash(img)),\n",
    "#     # 'phash': lambda img: str(imagehash.phash(img)),\n",
    "#     'dhash': lambda img: str(imagehash.dhash(img)),\n",
    "#     # 'whash': lambda img: str(imagehash.whash(img)),\n",
    "#     # 'colorhash': lambda img: str(imagehash.colorhash(img)),\n",
    "# }\n",
    "\n",
    "# def robust_df_save(df, filepath):\n",
    "#     \"\"\"Saves DataFrame to a CSV file robustly using a temporary file.\"\"\"\n",
    "#     temp_filepath = filepath + \".tmp\"\n",
    "#     try:\n",
    "#         df.to_csv(temp_filepath, index=False)\n",
    "#         # os.replace is generally atomic, shutil.move is a good fallback/alternative\n",
    "#         shutil.move(temp_filepath, filepath)\n",
    "#         print(f\"DataFrame successfully saved to {filepath}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving DataFrame to {filepath}: {e}\")\n",
    "#         if os.path.exists(temp_filepath):\n",
    "#             try:\n",
    "#                 os.remove(temp_filepath)\n",
    "#             except Exception as e_remove:\n",
    "#                 print(f\"Error removing temporary file {temp_filepath}: {e_remove}\")\n",
    "#         return False # Indicate failure\n",
    "#     return True # Indicate success\n",
    "\n",
    "# def calculate_hashes_for_image(image_path):\n",
    "#     \"\"\"\n",
    "#     Opens an image and calculates all specified hashes.\n",
    "#     Returns a dictionary with image name and its hashes, or an error dict.\n",
    "#     \"\"\"\n",
    "#     image_name = os.path.basename(image_path)\n",
    "#     try:\n",
    "#         img = Image.open(image_path).convert('RGB')\n",
    "#         hashes = {'image_name': image_name}\n",
    "#         for algo_name, hash_func in HASH_ALGORITHMS.items():\n",
    "#             try:\n",
    "#                 hashes[algo_name] = hash_func(img)\n",
    "#             except Exception: # e\n",
    "#                 # print(f\"\\nWarning: Could not compute {algo_name} for {image_path}: {e}\") # Commented for cleaner progress\n",
    "#                 hashes[algo_name] = None\n",
    "#         return hashes\n",
    "#     except FileNotFoundError:\n",
    "#         # print(f\"\\nError: Image file not found at {image_path}\") # Commented\n",
    "#         return {'image_name': image_name, 'error': 'File not found'}\n",
    "#     except Exception as e:\n",
    "#         # print(f\"\\nError processing image {image_path}: {e}\") # Commented\n",
    "#         return {'image_name': image_name, 'error': str(e)}\n",
    "\n",
    "# def find_png_images(folder_path):\n",
    "#     \"\"\"Scans the folder for PNG images.\"\"\"\n",
    "#     png_files = []\n",
    "#     for root, _, files in os.walk(folder_path):\n",
    "#         for file in files:\n",
    "#             if file.lower().endswith('.png'):\n",
    "#                 png_files.append(os.path.join(root, file))\n",
    "#     return png_files\n",
    "\n",
    "# def main():\n",
    "#     script_start_time = time.time()\n",
    "\n",
    "#     image_folder_input = input(f\"Enter path to image folder (default: {DEFAULT_IMAGE_FOLDER}): \")\n",
    "#     image_folder = image_folder_input if image_folder_input else DEFAULT_IMAGE_FOLDER\n",
    "\n",
    "#     if not os.path.isdir(image_folder):\n",
    "#         print(f\"Error: Folder not found: '{image_folder}'.\")\n",
    "#         return\n",
    "\n",
    "#     # --- Load Existing Data ---\n",
    "#     master_df = pd.DataFrame(columns=['Image Name', 'Hash Algorithm', 'Hash Value']) # Ensure columns exist even if empty\n",
    "#     processed_image_names = set()\n",
    "#     if os.path.exists(OUTPUT_CSV_FILE):\n",
    "#         print(f\"Found existing CSV: {OUTPUT_CSV_FILE}. Loading...\")\n",
    "#         try:\n",
    "#             master_df = pd.read_csv(OUTPUT_CSV_FILE)\n",
    "#             # Ensure essential columns exist; 'Duplicates' might or might not\n",
    "#             if 'Image Name' in master_df.columns:\n",
    "#                  processed_image_names = set(master_df['Image Name'].unique())\n",
    "#             print(f\"Loaded {len(processed_image_names)} unique image names from existing CSV.\")\n",
    "#             # Keep only essential columns for appending new hash data for now\n",
    "#             if 'Duplicates' in master_df.columns:\n",
    "#                 master_df = master_df[['Image Name', 'Hash Algorithm', 'Hash Value']]\n",
    "#         except pd.errors.EmptyDataError:\n",
    "#             print(f\"Warning: {OUTPUT_CSV_FILE} is empty. Starting fresh.\")\n",
    "#             master_df = pd.DataFrame(columns=['Image Name', 'Hash Algorithm', 'Hash Value'])\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {OUTPUT_CSV_FILE}: {e}. Starting with an empty dataset.\")\n",
    "#             master_df = pd.DataFrame(columns=['Image Name', 'Hash Algorithm', 'Hash Value'])\n",
    "#     else:\n",
    "#         print(f\"No existing CSV found at {OUTPUT_CSV_FILE}. A new one will be created.\")\n",
    "\n",
    "#     print(f\"Scanning for PNG images in '{image_folder}'...\")\n",
    "#     all_found_image_paths = find_png_images(image_folder)\n",
    "    \n",
    "#     image_paths_to_process = [\n",
    "#         p for p in all_found_image_paths if os.path.basename(p) not in processed_image_names\n",
    "#     ]\n",
    "#     total_images_to_process = len(image_paths_to_process)\n",
    "\n",
    "#     if not image_paths_to_process:\n",
    "#         print(\"No new PNG images found to process.\")\n",
    "#         if processed_image_names:\n",
    "#             print(\"Proceeding with duplicate analysis on existing data.\")\n",
    "#         else:\n",
    "#             print(\"No images to process and no existing data. Exiting.\")\n",
    "#             return\n",
    "#     else:\n",
    "#         print(f\"Found {len(all_found_image_paths)} total PNG images. Need to process {total_images_to_process} new images.\")\n",
    "#         print(f\"Starting hashing process using up to {MAX_WORKERS} workers...\")\n",
    "\n",
    "#         newly_computed_df_records = []\n",
    "#         processed_in_this_run_count = 0\n",
    "#         failed_during_this_run_count = 0\n",
    "        \n",
    "#         hashing_start_time = time.time()\n",
    "\n",
    "#         with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "#             for result_dict in executor.map(calculate_hashes_for_image, image_paths_to_process):\n",
    "#                 processed_in_this_run_count += 1\n",
    "#                 if 'error' in result_dict:\n",
    "#                     print(f\"\\nSkipped hashing for {result_dict['image_name']}: {result_dict['error']}\")\n",
    "#                     failed_during_this_run_count +=1\n",
    "#                 elif result_dict:\n",
    "#                     img_name = result_dict['image_name']\n",
    "#                     for algo_name in HASH_ALGORITHMS.keys():\n",
    "#                         hash_value = result_dict.get(algo_name)\n",
    "#                         if hash_value is not None:\n",
    "#                             newly_computed_df_records.append({\n",
    "#                                 'Image Name': img_name,\n",
    "#                                 'Hash Algorithm': algo_name,\n",
    "#                                 'Hash Value': hash_value\n",
    "#                             })\n",
    "                \n",
    "#                 # Progress printing\n",
    "#                 progress_percent = (processed_in_this_run_count / total_images_to_process) * 100 if total_images_to_process > 0 else 0\n",
    "#                 print(f\"Progress: {processed_in_this_run_count}/{total_images_to_process} new images ({progress_percent:.2f}%)\", end='\\r')\n",
    "\n",
    "#                 # Incremental save\n",
    "#                 if newly_computed_df_records and \\\n",
    "#                    (processed_in_this_run_count % SAVE_INTERVAL == 0 or processed_in_this_run_count == total_images_to_process):\n",
    "#                     print(f\"\\nSaving intermediate results... ({processed_in_this_run_count}/{total_images_to_process} processed)\")\n",
    "#                     temp_df = pd.DataFrame(newly_computed_df_records)\n",
    "#                     master_df = pd.concat([master_df, temp_df], ignore_index=True)\n",
    "#                     # Remove duplicates that might arise if script restarts and re-processes an image from a saved batch\n",
    "#                     master_df.drop_duplicates(subset=['Image Name', 'Hash Algorithm', 'Hash Value'], keep='first', inplace=True) # Or keep='last'\n",
    "                    \n",
    "#                     robust_df_save(master_df, OUTPUT_CSV_FILE)\n",
    "#                     newly_computed_df_records = [] # Reset for next batch\n",
    "        \n",
    "#         if total_images_to_process > 0:\n",
    "#             print() # Newline after progress bar\n",
    "\n",
    "#         hashing_duration = time.time() - hashing_start_time\n",
    "#         print(f\"Hashing of {processed_in_this_run_count - failed_during_this_run_count} new images (producing records) completed in {hashing_duration:.2f}s.\")\n",
    "#         if failed_during_this_run_count > 0:\n",
    "#             print(f\"{failed_during_this_run_count} images failed to process during this run.\")\n",
    "\n",
    "#     # --- Final DataFrame Preparation and Duplicate Analysis ---\n",
    "#     if master_df.empty and not os.path.exists(OUTPUT_CSV_FILE):\n",
    "#         print(\"No data to analyze or save. Exiting.\")\n",
    "#         return\n",
    "        \n",
    "#     # Ensure master_df is the most up-to-date from file, especially if no new images were processed\n",
    "#     if os.path.exists(OUTPUT_CSV_FILE):\n",
    "#         try:\n",
    "#             print(\"Loading final dataset for duplicate analysis...\")\n",
    "#             master_df = pd.read_csv(OUTPUT_CSV_FILE)\n",
    "#              # Ensure essential columns exist\n",
    "#             if 'Image Name' not in master_df.columns or 'Hash Algorithm' not in master_df.columns or 'Hash Value' not in master_df.columns:\n",
    "#                 print(\"CSV file is missing required columns. Cannot perform duplicate analysis.\")\n",
    "#                 return\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not reload CSV for final analysis: {e}. Attempting with in-memory data if available.\")\n",
    "#             if master_df.empty: # if in-memory also empty\n",
    "#                 return\n",
    "\n",
    "#     if master_df.empty:\n",
    "#         print(\"No hash data available for duplicate analysis. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Total unique images in dataset: {master_df['Image Name'].nunique()}\")\n",
    "#     print(\"Identifying duplicates based on all processed images...\")\n",
    "#     analysis_start_time = time.time()\n",
    "\n",
    "#     hashes_by_algo_and_value = {\n",
    "#         algo_name: master_df[master_df['Hash Algorithm'] == algo_name]\n",
    "#                    .groupby('Hash Value')['Image Name']\n",
    "#                    .apply(list)\n",
    "#                    .to_dict()\n",
    "#         for algo_name in HASH_ALGORITHMS.keys()\n",
    "#     }\n",
    "\n",
    "#     def find_row_duplicates(row):\n",
    "#         algo = row['Hash Algorithm']\n",
    "#         val = row['Hash Value']\n",
    "#         current_name = row['Image Name']\n",
    "#         if algo in hashes_by_algo_and_value and val in hashes_by_algo_and_value[algo]:\n",
    "#             all_with_hash = hashes_by_algo_and_value[algo][val]\n",
    "#             duplicates = [name for name in all_with_hash if name != current_name]\n",
    "#             return duplicates if duplicates else None\n",
    "#         return None\n",
    "\n",
    "#     master_df['Duplicates'] = master_df.apply(find_row_duplicates, axis=1)\n",
    "    \n",
    "#     print(f\"Duplicate analysis completed in {time.time() - analysis_start_time:.2f} seconds.\")\n",
    "\n",
    "#     print(\"\\nSaving final DataFrame with duplicate information...\")\n",
    "#     robust_df_save(master_df, OUTPUT_CSV_FILE)\n",
    "\n",
    "#     # --- Optional: Print summary of duplicates found ---\n",
    "#     print(\"\\n--- Duplicate Summary ---\")\n",
    "#     for algo_name in HASH_ALGORITHMS.keys():\n",
    "#         # print(f\"\\nAlgorithm: {algo_name}\")\n",
    "#         algo_df = master_df[master_df['Hash Algorithm'] == algo_name]\n",
    "#         duplicate_sets_for_algo = algo_df[algo_df['Duplicates'].apply(lambda x: x is not None and len(x) > 0)]\n",
    "        \n",
    "#         if not duplicate_sets_for_algo.empty:\n",
    "#             # Create unique sets of duplicate groups\n",
    "#             unique_groups = set()\n",
    "#             for _, row in duplicate_sets_for_algo.iterrows():\n",
    "#                 # A group includes the image itself and its duplicates, sorted for canonical representation\n",
    "#                 group = frozenset(sorted([row['Image Name']] + row['Duplicates']))\n",
    "#                 unique_groups.add(group)\n",
    "            \n",
    "#         #     if unique_groups:\n",
    "#         #         # for group in sorted(list(unique_groups), key=lambda s: (len(s), sorted(list(s))[0])): # Sort groups for consistent output\n",
    "#         #             # print(f\"  - Set of duplicates: {sorted(list(group))}\")\n",
    "#         #     else:\n",
    "#         #          print(\"  No unique duplicate sets found for this algorithm (after processing).\") # Should not happen if duplicate_sets_for_algo not empty\n",
    "#         # else:\n",
    "#         #     print(\"  No duplicates found for this algorithm.\")\n",
    "\n",
    "#     total_time = time.time() - script_start_time\n",
    "#     print(f\"\\nTotal script execution time: {total_time:.2f} seconds.\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6b529-d5b3-4ccd-be4a-ba541915ec1a",
   "metadata": {},
   "source": [
    "### Inspect duplicates csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dccf03c-2073-4475-b3a4-8b9156683d65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import ast # For safely evaluating string representations of lists\n",
    "# import io\n",
    "\n",
    "# df = pd.read_csv(\"image_hashes_and_duplicates.csv\")\n",
    "\n",
    "# # view only non Nan duplicates\n",
    "# df = df[df['Duplicates'].notna()]\n",
    "# df = df[df[\"Hash Algorithm\"].isin([\"sha256\", \"dhash\"])]\n",
    "\n",
    "# print(df['Hash Algorithm'].value_counts())\n",
    "\n",
    "# # GET list of duplicate images\n",
    "\n",
    "# def get_final_duplicate_groups(df: pd.DataFrame) -> list[list[str]]:\n",
    "#     \"\"\"\n",
    "#     Processes a DataFrame containing image duplication information to find\n",
    "#     unique, consolidated groups of duplicate images.\n",
    "\n",
    "#     Args:\n",
    "#         df: Pandas DataFrame with columns including 'Image Name' and 'Duplicates'.\n",
    "#             'Duplicates' column should contain a string representation of a list\n",
    "#             of duplicate image filenames, or be NaN/empty if no duplicates\n",
    "#             were found by that specific hash.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of lists, where each inner list contains unique image filenames\n",
    "#         that form a duplicate group. Inner lists are sorted, and the outer\n",
    "#         list is sorted by the first element of each inner list for consistency.\n",
    "#     \"\"\"\n",
    "#     initial_groups = []\n",
    "\n",
    "#     for index, row in df.iterrows():\n",
    "#         current_image = row['Image Name']\n",
    "#         duplicates_val = row['Duplicates']\n",
    "#         parsed_duplicates_list = []\n",
    "\n",
    "#         if pd.notna(duplicates_val):\n",
    "#             if isinstance(duplicates_val, str):\n",
    "#                 # Clean up the string: remove potential outer quotes if any, then strip whitespace\n",
    "#                 temp_str = duplicates_val.strip()\n",
    "#                 if temp_str.startswith('\"') and temp_str.endswith('\"'): # Handle if CSV read quotes as part of string\n",
    "#                     temp_str = temp_str[1:-1]\n",
    "#                 temp_str = temp_str.strip()\n",
    "                \n",
    "#                 if temp_str and temp_str != \"[]\": # Ensure it's not empty or just \"[]\"\n",
    "#                     try:\n",
    "#                         evaluated_val = ast.literal_eval(temp_str)\n",
    "#                         if isinstance(evaluated_val, list):\n",
    "#                             # Filter to ensure all items in the list are strings (filenames)\n",
    "#                             parsed_duplicates_list = [item for item in evaluated_val if isinstance(item, str)]\n",
    "#                     except (ValueError, SyntaxError, TypeError):\n",
    "#                         print(f\"Warning: Could not parse 'Duplicates' string: '{duplicates_val}' for image '{current_image}' at index {index}. Skipping this entry for group formation.\")\n",
    "#             elif isinstance(duplicates_val, list): # If it's already a list (e.g., if DataFrame was constructed differently)\n",
    "#                  parsed_duplicates_list = [item for item in duplicates_val if isinstance(item, str)]\n",
    "\n",
    "\n",
    "#         # Create a group including the current image and its parsed duplicates\n",
    "#         # Only proceed if there are actual duplicates found for this entry, or if the group would be non-trivial\n",
    "#         current_group_set = set()\n",
    "#         if isinstance(current_image, str): # Ensure current_image is a string\n",
    "#             current_group_set.add(current_image)\n",
    "        \n",
    "#         if parsed_duplicates_list:\n",
    "#             current_group_set.update(parsed_duplicates_list)\n",
    "\n",
    "#         # Only add the group if it contains more than one image (i.e., actual duplicates)\n",
    "#         # or if it's a single image that might merge with other groups later.\n",
    "#         # For this problem, we are interested in sets formed by row['Image Name'] + row['Duplicates']\n",
    "#         if parsed_duplicates_list: # Meaning this row explicitly states duplicates\n",
    "#             initial_groups.append(current_group_set)\n",
    "\n",
    "\n",
    "#     if not initial_groups:\n",
    "#         return []\n",
    "\n",
    "#     # Merge overlapping sets\n",
    "#     # Algorithm: Iterate through initial_groups. For each group, try to merge it with existing\n",
    "#     #            groups in final_merged_sets. If it overlaps with multiple, merge all involved.\n",
    "#     final_merged_sets = []\n",
    "#     for group_to_add in initial_groups:\n",
    "#         if not group_to_add: # Skip empty sets if any were formed\n",
    "#             continue\n",
    "            \n",
    "#         overlapping_indices = []\n",
    "#         for i, existing_set in enumerate(final_merged_sets):\n",
    "#             if not group_to_add.isdisjoint(existing_set): # Check for intersection\n",
    "#                 overlapping_indices.append(i)\n",
    "        \n",
    "#         if not overlapping_indices:\n",
    "#             # No overlap, add as a new set\n",
    "#             final_merged_sets.append(group_to_add.copy())\n",
    "#         else:\n",
    "#             # Overlaps with one or more existing sets\n",
    "#             merged_set = group_to_add.copy()\n",
    "#             # Iterate in reverse to safely pop elements\n",
    "#             for i in sorted(overlapping_indices, reverse=True):\n",
    "#                 merged_set.update(final_merged_sets.pop(i))\n",
    "#             final_merged_sets.append(merged_set)\n",
    "            \n",
    "#     # Convert sets to sorted lists\n",
    "#     result_list_of_lists = [sorted(list(s)) for s in final_merged_sets if s] # Ensure no empty sets slip through\n",
    "\n",
    "#     # Sort the outer list for deterministic output (e.g., by the first element of each inner list)\n",
    "#     result_list_of_lists.sort(key=lambda x: x[0] if x else \"\")\n",
    "\n",
    "#     return result_list_of_lists\n",
    "\n",
    "# # Load the example data into a DataFrame\n",
    "# df_example = df\n",
    "\n",
    "# # Get the consolidated duplicate groups\n",
    "# final_duplicate_groups = list(get_final_duplicate_groups(df_example.copy())) # Use .copy() if df might be modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb6632-5e95-4ce3-88a2-9815d2869413",
   "metadata": {},
   "source": [
    "## Move unecessary duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7049021c-b030-4d47-bfd7-76164a8277b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# # --- Configuration ---\n",
    "# # TODO: Update these paths before running\n",
    "# SOURCE_IMAGE_DIR = \"./output\" # e.g., \"data/all_images\"\n",
    "# DUPLICATES_DEST_DIR = \"./duplicates\" # e.g., \"data/moved_duplicates\"\n",
    "\n",
    "# # Example: final_duplicate_groups (replace with your actual list)\n",
    "# # final_duplicate_groups = [\n",
    "# #  ['0013dfe43ba6a59c437cc173685a3b6e.png', '7f58a1ef040408dfdf71ff82ec6cb43d.png'],\n",
    "# #  ['001b9ee7bf40b962c1019fd029f6ce87.png', '1ff9e419afa6db9ab683b5e1846bc803.png', 'bc03f1381312769a5dd493d989779562.png'],\n",
    "# #  # ... more groups\n",
    "# # ]\n",
    "# # Make sure 'final_duplicate_groups' is defined in your notebook before this script.\n",
    "\n",
    "\n",
    "# def get_json_path(image_filename: str, directory: str) -> str:\n",
    "#     \"\"\"Constructs the path to the JSON file corresponding to an image.\"\"\"\n",
    "#     base_name, _ = os.path.splitext(image_filename)\n",
    "#     return os.path.join(directory, base_name + \".json\")\n",
    "\n",
    "# def load_json_data(json_filepath: str) -> dict | None:\n",
    "#     \"\"\"Loads JSON data from a file.\"\"\"\n",
    "#     if not os.path.exists(json_filepath):\n",
    "#         print(f\"Warning: JSON file not found: {json_filepath}\")\n",
    "#         return None\n",
    "#     try:\n",
    "#         with open(json_filepath, 'r') as f:\n",
    "#             return json.load(f)\n",
    "#     except json.JSONDecodeError:\n",
    "#         print(f\"Warning: Could not decode JSON from file: {json_filepath}\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Warning: Error loading JSON file {json_filepath}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# def is_label_descriptive(label: str | None) -> bool:\n",
    "#     \"\"\"Checks if an image class label is descriptive.\"\"\"\n",
    "#     if label is None:\n",
    "#         return False # No label is not descriptive\n",
    "#     label_lower = label.lower()\n",
    "#     if label_lower == \"0_unknown\" or label_lower.endswith(\"_unknown\"):\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# def get_json_informativeness(json_data: dict | None) -> tuple[int, bool, str | None]:\n",
    "#     \"\"\"\n",
    "#     Evaluates the informativeness of a JSON file.\n",
    "#     Returns a tuple: (number_of_shapes, is_descriptive_label, image_class_label).\n",
    "#     Higher number of shapes is better. Descriptive label is better.\n",
    "#     \"\"\"\n",
    "#     if json_data is None:\n",
    "#         return (0, False, None) # Least informative\n",
    "\n",
    "#     num_shapes = len(json_data.get(\"shapes\", []))\n",
    "#     image_class_label = json_data.get(\"image_class_label\")\n",
    "#     descriptive_label = is_label_descriptive(image_class_label)\n",
    "    \n",
    "#     return (num_shapes, descriptive_label, image_class_label)\n",
    "\n",
    "# def move_file_pair(image_filename: str, source_dir: str, dest_dir: str):\n",
    "#     \"\"\"Moves an image and its corresponding JSON file.\"\"\"\n",
    "#     base_name, img_ext = os.path.splitext(image_filename)\n",
    "#     json_filename = base_name + \".json\"\n",
    "\n",
    "#     source_img_path = os.path.join(source_dir, image_filename)\n",
    "#     source_json_path = os.path.join(source_dir, json_filename)\n",
    "#     dest_img_path = os.path.join(dest_dir, image_filename)\n",
    "#     dest_json_path = os.path.join(dest_dir, json_filename)\n",
    "\n",
    "#     files_moved_count = 0\n",
    "#     if os.path.exists(source_img_path):\n",
    "#         try:\n",
    "#             shutil.move(source_img_path, dest_img_path)\n",
    "#             # print(f\"Moved image: {image_filename} to {dest_dir}\")\n",
    "#             files_moved_count +=1\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error moving image {source_img_path}: {e}\")\n",
    "#     else:\n",
    "#         print(f\"Warning: Source image not found, cannot move: {source_img_path}\")\n",
    "        \n",
    "#     if os.path.exists(source_json_path):\n",
    "#         try:\n",
    "#             shutil.move(source_json_path, dest_json_path)\n",
    "#             # print(f\"Moved JSON: {json_filename} to {dest_dir}\")\n",
    "#             files_moved_count +=1\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error moving JSON {source_json_path}: {e}\")\n",
    "#     else:\n",
    "#         # This might be acceptable if the image didn't have a JSON, or it was already handled\n",
    "#         print(f\"Warning: Source JSON not found, cannot move: {source_json_path}\")\n",
    "#     return files_moved_count\n",
    "\n",
    "\n",
    "# def process_duplicate_groups(duplicate_groups: list[list[str]], source_dir: str, duplicates_dir: str):\n",
    "#     \"\"\"\n",
    "#     Processes groups of duplicate images, keeps the most informative one,\n",
    "#     and moves the others to the duplicates_dir.\n",
    "#     \"\"\"\n",
    "#     if not os.path.exists(duplicates_dir):\n",
    "#         os.makedirs(duplicates_dir)\n",
    "#         print(f\"Created duplicates directory: {duplicates_dir}\")\n",
    "\n",
    "#     if not os.path.exists(source_dir):\n",
    "#         print(f\"Error: Source directory '{source_dir}' does not exist. Please check the path.\")\n",
    "#         return\n",
    "        \n",
    "#     total_files_to_move = 0\n",
    "#     total_files_moved = 0\n",
    "\n",
    "#     for i, group in enumerate(duplicate_groups):\n",
    "#         print(f\"\\nProcessing group {i+1}/{len(duplicate_groups)}: {group}\")\n",
    "#         if not group or len(group) < 2:\n",
    "#             print(f\"Skipping group {i+1} as it has less than two items or is empty.\")\n",
    "#             continue\n",
    "\n",
    "#         best_image_filename = None\n",
    "#         # Initialize with least informative: (num_shapes, is_descriptive, label_string)\n",
    "#         best_informativeness = (-1, False, None) \n",
    "\n",
    "#         candidate_info = []\n",
    "\n",
    "#         for image_filename in group:\n",
    "#             json_filepath = get_json_path(image_filename, source_dir)\n",
    "#             json_data = load_json_data(json_filepath)\n",
    "            \n",
    "#             if json_data is None and not os.path.exists(json_filepath):\n",
    "#                 # If JSON doesn't exist, it's less informative than one that does.\n",
    "#                 # Still, we need to track it to potentially keep the image if all others also lack JSONs\n",
    "#                 # or if it's the only one left by some chance.\n",
    "#                 # This case is treated as (0 shapes, not descriptive).\n",
    "#                 current_informativeness = (0, False, None)\n",
    "#                 print(f\"  Image '{image_filename}': No JSON found or JSON is invalid. Treated as least informative.\")\n",
    "#             elif json_data is None and os.path.exists(json_filepath):\n",
    "#                  current_informativeness = (0, False, None) # JSON exists but is invalid\n",
    "#                  print(f\"  Image '{image_filename}': JSON found but invalid. Treated as least informative.\")\n",
    "#             else:\n",
    "#                 current_informativeness = get_json_informativeness(json_data)\n",
    "#                 print(f\"  Image '{image_filename}': Shapes={current_informativeness[0]}, DescriptiveLabel={current_informativeness[1]}, Label='{current_informativeness[2]}'\")\n",
    "\n",
    "#             candidate_info.append({\n",
    "#                 \"filename\": image_filename,\n",
    "#                 \"informativeness\": current_informativeness,\n",
    "#                 \"json_exists_and_valid\": json_data is not None\n",
    "#             })\n",
    "\n",
    "#         # Determine the best candidate\n",
    "#         # The first image in the original `group` list acts as a tie-breaker if informativeness is identical.\n",
    "#         # We iterate through candidates in the order they appeared in the `group`.\n",
    "        \n",
    "#         # Initialize best_image_filename with the first item, in case all have no JSON or are equally bad.\n",
    "#         best_image_filename = candidate_info[0][\"filename\"]\n",
    "#         best_informativeness = candidate_info[0][\"informativeness\"]\n",
    "#         best_json_valid = candidate_info[0][\"json_exists_and_valid\"]\n",
    "\n",
    "#         for candidate in candidate_info[1:]: # Start from the second candidate\n",
    "#             current_filename = candidate[\"filename\"]\n",
    "#             current_informativeness = candidate[\"informativeness\"]\n",
    "#             current_json_valid = candidate[\"json_exists_and_valid\"]\n",
    "\n",
    "#             # Priority:\n",
    "#             # 1. More shapes is better.\n",
    "#             # 2. Descriptive label is better than non-descriptive/missing.\n",
    "#             # 3. If all above are equal, prefer one with a valid JSON over one without.\n",
    "#             # 4. If all above are equal, the first one encountered (implicit from loop order) is kept.\n",
    "\n",
    "#             if current_informativeness[0] > best_informativeness[0]: # More shapes\n",
    "#                 best_image_filename = current_filename\n",
    "#                 best_informativeness = current_informativeness\n",
    "#                 best_json_valid = current_json_valid\n",
    "#             elif current_informativeness[0] == best_informativeness[0]: # Equal shapes\n",
    "#                 if current_informativeness[1] and not best_informativeness[1]: # Current is descriptive, best is not\n",
    "#                     best_image_filename = current_filename\n",
    "#                     best_informativeness = current_informativeness\n",
    "#                     best_json_valid = current_json_valid\n",
    "#                 elif current_informativeness[1] == best_informativeness[1]: # Both descriptive or both non-descriptive\n",
    "#                     # If labels are equivalent in descriptiveness, prefer one with a valid JSON\n",
    "#                     if current_json_valid and not best_json_valid:\n",
    "#                         best_image_filename = current_filename\n",
    "#                         best_informativeness = current_informativeness\n",
    "#                         best_json_valid = current_json_valid\n",
    "#                     # If both have valid JSONs or both don't, and labels/shapes are equal,\n",
    "#                     # the existing 'best_image_filename' (which was earlier in the list) is kept.\n",
    "\n",
    "\n",
    "#         print(f\"  -> Keeping: '{best_image_filename}' (Shapes={best_informativeness[0]}, Descriptive={best_informativeness[1]}, Label='{best_informativeness[2]}')\")\n",
    "\n",
    "#         # Move other files in the group\n",
    "#         for image_filename in group:\n",
    "#             if image_filename != best_image_filename:\n",
    "#                 print(f\"  -> Moving: '{image_filename}'\")\n",
    "#                 total_files_to_move += 2 # For image and JSON\n",
    "#                 moved_count = move_file_pair(image_filename, source_dir, duplicates_dir)\n",
    "#                 total_files_moved += moved_count\n",
    "#             else:\n",
    "#                 # Ensure the JSON for the \"best\" image actually exists if we determined it should.\n",
    "#                 # This is more of a sanity check, as 'load_json_data' handles non-existence.\n",
    "#                 best_json_path = get_json_path(best_image_filename, source_dir)\n",
    "#                 if best_json_valid and not os.path.exists(best_json_path):\n",
    "#                      print(f\"Warning: The selected best image '{best_image_filename}' was expected to have a valid JSON, but '{best_json_path}' was not found during the move phase. This shouldn't happen if logic is correct.\")\n",
    "#                 elif not best_json_valid and os.path.exists(best_json_path):\n",
    "#                      # This could happen if the JSON was present but unreadable. It will remain.\n",
    "#                      pass\n",
    "\n",
    "\n",
    "#     print(f\"\\n--- Processing Complete ---\")\n",
    "#     print(f\"Attempted to move up to {total_files_to_move} files (images and JSONs).\")\n",
    "#     print(f\"Actually moved {total_files_moved} files.\")\n",
    "#     remaining_files_in_source = len([name for name in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, name))])\n",
    "#     files_in_duplicates_dir = len([name for name in os.listdir(duplicates_dir) if os.path.isfile(os.path.join(duplicates_dir, name))])\n",
    "#     print(f\"Files remaining in source directory ('{source_dir}'): {remaining_files_in_source}\")\n",
    "#     print(f\"Files in duplicates directory ('{duplicates_dir}'): {files_in_duplicates_dir}\")\n",
    "\n",
    "\n",
    "# # --- Main execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     # This is where you would load your `final_duplicate_groups`\n",
    "#     # For testing, you can create a dummy CSV and DataFrame like in your example,\n",
    "#     # or directly define `final_duplicate_groups`.\n",
    "\n",
    "#     # Ensure `final_duplicate_groups` is defined.\n",
    "#     # Example (if you're not running this directly in the notebook after your code):\n",
    "#     # final_duplicate_groups = [\n",
    "#     #     ['image1_dupA.png', 'image1_dupB.png', 'image1_original.png'],\n",
    "#     #     ['image2_better.png', 'image2_worse.png']\n",
    "#     # ]\n",
    "#     # Make sure SOURCE_IMAGE_DIR contains dummy files like:\n",
    "#     # image1_dupA.png, image1_dupA.json\n",
    "#     # image1_dupB.png, image1_dupB.json\n",
    "#     # image1_original.png, image1_original.json\n",
    "#     # etc. with varying JSON content.\n",
    "\n",
    "#     # --- RUN THE SCRIPT ---\n",
    "#     # Ensure `final_duplicate_groups` is defined from your previous notebook cell\n",
    "#     if 'final_duplicate_groups' in locals() or 'final_duplicate_groups' in globals():\n",
    "#         if not SOURCE_IMAGE_DIR.startswith(\"path/to/your\") and not DUPLICATES_DEST_DIR.startswith(\"path/to/your\"):\n",
    "#             process_duplicate_groups(final_duplicate_groups, SOURCE_IMAGE_DIR, DUPLICATES_DEST_DIR)\n",
    "#         else:\n",
    "#             print(\"ERROR: Please update SOURCE_IMAGE_DIR and DUPLICATES_DEST_DIR variables in the script.\")\n",
    "#             print(\"SOURCE_IMAGE_DIR is currently:\", SOURCE_IMAGE_DIR)\n",
    "#             print(\"DUPLICATES_DEST_DIR is currently:\", DUPLICATES_DEST_DIR)\n",
    "\n",
    "#     else:\n",
    "#         print(\"Error: `final_duplicate_groups` is not defined. Please ensure it's generated before running this script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a687171-3d69-4397-a3b1-b7eca1ca50ad",
   "metadata": {},
   "source": [
    "## show duplicate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b526998-bded-4488-b4be-41876b71dbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image, UnidentifiedImageError # For loading images and handling errors\n",
    "# import numpy as np # For more robust Axes handling\n",
    "\n",
    "# def visualize_duplicate_groups(\n",
    "#     duplicate_groups: list[list[str]],\n",
    "#     image_folder_path: str,\n",
    "#     max_images_per_row: int = 4,\n",
    "#     max_groups_to_display: int = None,\n",
    "#     figsize_per_image: tuple[float, float] = (3, 3)\n",
    "# ) -> None:\n",
    "#     \"\"\"\n",
    "#     Visualizes groups of duplicate images by displaying them in a grid.\n",
    "\n",
    "#     Args:\n",
    "#         duplicate_groups: A list of lists, where each inner list contains\n",
    "#                           filenames of images considered duplicates.\n",
    "#         image_folder_path: The base path to the folder containing the image files.\n",
    "#         max_images_per_row: Maximum number of images to display in a single row\n",
    "#                             within the grid for each group.\n",
    "#         max_groups_to_display: Optional. The maximum number of duplicate groups\n",
    "#                                to display. If None, all groups are processed.\n",
    "#                                This is useful to prevent opening too many plot windows.\n",
    "#         figsize_per_image: Tuple (width, height) in inches for each individual\n",
    "#                            image subplot. The total figure size will be scaled based\n",
    "#                            on this and the number of images.\n",
    "#     \"\"\"\n",
    "#     if not duplicate_groups:\n",
    "#         print(\"No duplicate groups to visualize.\")\n",
    "#         return\n",
    "\n",
    "#     num_groups_actually_displayed = 0\n",
    "#     for i, group in enumerate(duplicate_groups):\n",
    "#         if max_groups_to_display is not None and num_groups_actually_displayed >= max_groups_to_display:\n",
    "#             print(f\"\\nReached maximum number of groups to display ({max_groups_to_display}).\")\n",
    "#             break\n",
    "\n",
    "#         if not group or len(group) < 2:\n",
    "#             # Optionally print a message if you want to know about skipped small/empty groups\n",
    "#             # print(f\"Skipping Group {i+1}: Contains fewer than 2 images or is empty.\")\n",
    "#             continue\n",
    "\n",
    "#         print(f\"\\nProcessing Duplicate Group {i+1} (contains {len(group)} candidate images):\")\n",
    "\n",
    "#         images_to_display = []\n",
    "#         image_titles = []\n",
    "#         loaded_image_objects = [] # To keep track of PIL Image objects for closing\n",
    "\n",
    "#         for filename in group:\n",
    "#             img_path = os.path.join(image_folder_path, filename)\n",
    "#             try:\n",
    "#                 if not os.path.exists(img_path):\n",
    "#                     print(f\"  WARNING: Image file not found at '{img_path}'. Skipping.\")\n",
    "#                     continue\n",
    "#                 img = Image.open(img_path)\n",
    "#                 images_to_display.append(img.copy()) # Work with a copy\n",
    "#                 image_titles.append(filename)\n",
    "#                 loaded_image_objects.append(img) # Add original to close later\n",
    "#             except UnidentifiedImageError:\n",
    "#                 print(f\"  WARNING: Could not open or identify image '{img_path}'. It might be corrupted or not a valid image format. Skipping.\")\n",
    "#             except FileNotFoundError: # Should be caught by os.path.exists, but as a safeguard\n",
    "#                 print(f\"  WARNING: Image file not found (FileNotFoundError) at '{img_path}'. Skipping.\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  WARNING: An unexpected error occurred loading image '{img_path}': {e}. Skipping.\")\n",
    "\n",
    "#         if len(images_to_display) < 1:\n",
    "#             print(f\"  No valid images could be loaded for Group {i+1}. Skipping visualization for this group.\")\n",
    "#             # Close any images that were successfully loaded before this group was skipped\n",
    "#             for img_obj in loaded_image_objects:\n",
    "#                 img_obj.close()\n",
    "#             continue\n",
    "\n",
    "#         num_images = len(images_to_display)\n",
    "#         n_cols = min(num_images, max_images_per_row)\n",
    "#         n_cols = max(1, n_cols) # Ensure at least 1 column\n",
    "#         n_rows = (num_images + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "#         fig_width = n_cols * figsize_per_image[0]\n",
    "#         fig_height = n_rows * figsize_per_image[1]\n",
    "\n",
    "#         fig, axes = plt.subplots(n_rows, n_cols, figsize=(fig_width, fig_height), squeeze=False)\n",
    "#         # 'squeeze=False' ensures 'axes' is always a 2D array, simplifying indexing.\n",
    "\n",
    "#         fig.suptitle(f\"Duplicate Group {i+1} ({num_images} of {len(group)} images shown)\", fontsize=16)\n",
    "\n",
    "#         ax_flat = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "#         for idx, (img_data, title) in enumerate(zip(images_to_display, image_titles)):\n",
    "#             if idx < len(ax_flat):\n",
    "#                 ax_flat[idx].imshow(img_data)\n",
    "#                 ax_flat[idx].set_title(title, fontsize=10)\n",
    "#                 ax_flat[idx].axis('off')  # Hide axis ticks and labels\n",
    "\n",
    "#         # Turn off unused subplots if the grid is not perfectly filled\n",
    "#         for idx in range(num_images, len(ax_flat)):\n",
    "#             ax_flat[idx].axis('off')\n",
    "\n",
    "#         plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # Adjust layout to make space for suptitle\n",
    "#         plt.show()  # In a script, this blocks until the window is closed.\n",
    "#                     # In Jupyter Notebook, displays inline.\n",
    "\n",
    "#         num_groups_actually_displayed += 1\n",
    "\n",
    "#         # Close all PIL Image objects for the current group after displaying\n",
    "#         for img_obj in loaded_image_objects:\n",
    "#             img_obj.close()\n",
    "#         for img_data in images_to_display: # If img.copy() was used and stored here\n",
    "#             if hasattr(img_data, 'close'):\n",
    "#                 img_data.close()\n",
    "\n",
    "\n",
    "#     if num_groups_actually_displayed == 0 and any(g for g in duplicate_groups if len(g or []) >=2):\n",
    "#         print(\"\\nNo groups were displayed. This might be due to all groups having fewer than 2 images, issues loading all images, or all groups being skipped.\")\n",
    "#     elif num_groups_actually_displayed > 0 :\n",
    "#         print(f\"\\nDisplayed {num_groups_actually_displayed} group(s).\")\n",
    "\n",
    "# # --- How to use this function with your data ---\n",
    "# #\n",
    "# # 1. Make sure the function `visualize_duplicate_groups` is defined (as above).\n",
    "# #\n",
    "# # 2. You need to have your `final_duplicate_groups` variable populated.\n",
    "# #    This variable should be a list of lists, where each inner list\n",
    "# #    contains the filenames of images identified as duplicates.\n",
    "# #    For example:\n",
    "# #    final_duplicate_groups = [\n",
    "# #        ['imageA_01.png', 'imageA_02.png', 'imageA_03.png'], # Group 1\n",
    "# #        ['imageB_01.png', 'imageB_02.png'],                 # Group 2\n",
    "# #        # ... more groups\n",
    "# #    ]\n",
    "# #\n",
    "# # 3. You need to specify the path to the folder where these images are stored.\n",
    "# #    For example:\n",
    "# #    my_actual_image_folder_path = \"/path/to/your/image_directory\"\n",
    "# #    # Or on Windows:\n",
    "# #    # my_actual_image_folder_path = r\"C:\\path\\to\\your\\image_directory\"\n",
    "# #\n",
    "# # 4. Call the function:\n",
    "# #    if 'final_duplicate_groups' in locals() and 'my_actual_image_folder_path' in locals():\n",
    "# #        if final_duplicate_groups and my_actual_image_folder_path:\n",
    "# #            print(\"Starting visualization of your duplicate image groups...\")\n",
    "# #            visualize_duplicate_groups(\n",
    "# #                final_duplicate_groups,\n",
    "# #                my_actual_image_folder_path,\n",
    "# #                max_images_per_row=3,        # Optional: Adjust as needed\n",
    "# #                max_groups_to_display=None,  # Optional: Set to a number e.g. 5, or None for all\n",
    "# #                figsize_per_image=(3, 3)     # Optional: Adjust as needed\n",
    "# #            )\n",
    "# #        else:\n",
    "# #            print(\"Please ensure 'final_duplicate_groups' is not empty and 'my_actual_image_folder_path' is set.\")\n",
    "# #    else:\n",
    "# #        print(\"Please define 'final_duplicate_groups' and 'my_actual_image_folder_path' before calling the visualization.\")\n",
    "\n",
    "# # Ensure the 'visualize_duplicate_groups' function is defined above this point.\n",
    "# # import os # Already imported within visualize_duplicate_groups if needed there,\n",
    "#            # but good practice to have it if you use os.path.exists here too.\n",
    "# # import matplotlib.pyplot as plt # Already imported within visualize_duplicate_groups\n",
    "# # from PIL import Image, UnidentifiedImageError # Already imported\n",
    "# # import numpy as np # Already imported\n",
    "\n",
    "# # --- STEP 1: Assume your 'final_duplicate_groups' object is already populated ---\n",
    "# # This object should be a list of lists of image filenames, for example:\n",
    "# #\n",
    "# # final_duplicate_groups = [\n",
    "# #     ['image_A_copy1.jpg', 'image_A_original.jpg', 'image_A_variant.png'],\n",
    "# #     ['photo_XYZ.png', 'photo_XYZ_duplicate.png'],\n",
    "# #     # ... more groups ...\n",
    "# # ]\n",
    "# #\n",
    "# # Make sure this variable exists and contains the data from your previous processing steps.\n",
    "\n",
    "\n",
    "# # --- STEP 2: Define the path to the folder containing your actual images ---\n",
    "# #\n",
    "# # !! IMPORTANT !!\n",
    "# # You MUST replace the placeholder path below with the ACTUAL path\n",
    "# # to the directory where your image files are stored on your computer.\n",
    "# #\n",
    "# # For example, if your images are in \"/home/user/pictures/my_project_images\":\n",
    "# # my_image_directory = \"/home/user/pictures/my_project_images\"\n",
    "# #\n",
    "# # Or, if on Windows, they are in \"D:\\photos\\dataset\":\n",
    "# # my_image_directory = r\"D:\\photos\\dataset\"\n",
    "\n",
    "# my_image_directory = \"./output\"\n",
    "\n",
    "\n",
    "# # --- STEP 3: Call the visualization function ---\n",
    "\n",
    "# # Basic check to ensure the variables are ready\n",
    "# if 'final_duplicate_groups' in locals() and isinstance(final_duplicate_groups, list):\n",
    "#     if my_image_directory == \"/replace/with/actual/path/to/your/image_folder\":\n",
    "#         print(\"⚠️ WARNING: Please update 'my_image_directory' with the actual path to your image folder.\")\n",
    "#     elif not os.path.exists(my_image_directory):\n",
    "#         print(f\"⛔ ERROR: The specified image folder path does not exist: {my_image_directory}\")\n",
    "#         print(\"Please ensure 'my_image_directory' points to the correct location of your images.\")\n",
    "#     elif not final_duplicate_groups:\n",
    "#         print(\"Information: 'final_duplicate_groups' is empty. No duplicate groups to visualize.\")\n",
    "#     else:\n",
    "#         print(f\"Starting visualization for image groups found in: {my_image_directory}\")\n",
    "#         visualize_duplicate_groups(\n",
    "#             duplicate_groups=final_duplicate_groups,\n",
    "#             image_folder_path=my_image_directory,\n",
    "#             max_images_per_row=3,        # Optional: Adjust how many images appear side-by-side\n",
    "#             max_groups_to_display=20,     # Optional: Limit to e.g., first 10 groups, or None for all\n",
    "#             figsize_per_image=(3, 3)     # Optional: Adjust the display size of each image\n",
    "#         )\n",
    "# else:\n",
    "#     print(\"⛔ ERROR: 'final_duplicate_groups' is not defined or is not a list.\")\n",
    "#     print(\"       Please ensure your previous code successfully created this object.\")\n",
    "#     print(\"       If 'final_duplicate_groups' is defined, ensure it's a list of lists as expected.\")\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89453a6-5e22-4df0-82d4-6ecf6f49418a",
   "metadata": {},
   "source": [
    "### Move json files with no images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7dd4cf9-c3f4-43ed-a25e-683ad39ca18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning './output' to build file lists (optimized)...\n",
      "Found 54421 unique image base names.\n",
      "Found 54421 JSON files to check.\n",
      "\n",
      "--- Orphaned JSON Scan Complete (Optimized) ---\n",
      "Total JSON files checked: 54421.\n",
      "Identified 0 as potentially orphaned.\n",
      "Successfully moved 0 orphaned JSON files to './duplicates'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Configuration ---\n",
    "# TODO: Update these paths before running\n",
    "SOURCE_DIR = \"./output\"  # Directory to scan\n",
    "ORPHANED_JSON_DEST_DIR = \"./duplicates\" # Directory to move orphaned JSONs\n",
    "\n",
    "# Common image extensions to check for (ensure lowercase)\n",
    "IMAGE_EXTENSIONS = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp']\n",
    "\n",
    "def move_orphaned_json_files_optimized(source_directory: str, orphaned_dest_directory: str, img_extensions: list[str]):\n",
    "    \"\"\"\n",
    "    Moves JSON files from source_directory to orphaned_dest_directory\n",
    "    if no corresponding image file (with a common extension) exists.\n",
    "    This version is optimized to reduce file system checks.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source_directory):\n",
    "        print(f\"Error: Source directory '{source_directory}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(orphaned_dest_directory):\n",
    "        try:\n",
    "            os.makedirs(orphaned_dest_directory)\n",
    "            print(f\"Created directory for orphaned JSONs: '{orphaned_dest_directory}'\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error: Could not create destination directory '{orphaned_dest_directory}': {e}\")\n",
    "            return\n",
    "\n",
    "    image_basenames = set()\n",
    "    json_files_to_process = []\n",
    "\n",
    "    print(f\"\\nScanning '{source_directory}' to build file lists (optimized)...\")\n",
    "\n",
    "    # First pass: Collect all image basenames (without extension, lowercase)\n",
    "    # and identify all JSON files (original case for moving).\n",
    "    all_files = []\n",
    "    try:\n",
    "        all_files = os.listdir(source_directory)\n",
    "    except OSError as e:\n",
    "        print(f\"Error listing files in '{source_directory}': {e}\")\n",
    "        return\n",
    "\n",
    "    for filename in all_files:\n",
    "        # Normalize to lowercase for extension and basename comparison\n",
    "        name_lower = filename.lower()\n",
    "        base_name_lower, ext_lower = os.path.splitext(name_lower)\n",
    "\n",
    "        if ext_lower in img_extensions:\n",
    "            # Check if the file actually exists (os.listdir might return names of deleted files in some edge cases)\n",
    "            # and is a file (not a directory with an image-like extension)\n",
    "            if os.path.isfile(os.path.join(source_directory, filename)):\n",
    "                 image_basenames.add(base_name_lower)\n",
    "        elif ext_lower == \".json\":\n",
    "            if os.path.isfile(os.path.join(source_directory, filename)):\n",
    "                json_files_to_process.append(filename) # Store original filename\n",
    "\n",
    "    print(f\"Found {len(image_basenames)} unique image base names.\")\n",
    "    print(f\"Found {len(json_files_to_process)} JSON files to check.\")\n",
    "\n",
    "    moved_count = 0\n",
    "    orphaned_json_found_count = 0\n",
    "\n",
    "    # Second pass: Check JSON files against the image basenames set\n",
    "    for json_filename in json_files_to_process:\n",
    "        json_base_name_lower, _ = os.path.splitext(json_filename.lower())\n",
    "\n",
    "        if json_base_name_lower not in image_basenames:\n",
    "            orphaned_json_found_count += 1\n",
    "            source_json_path = os.path.join(source_directory, json_filename)\n",
    "            dest_json_path = os.path.join(orphaned_dest_directory, json_filename)\n",
    "            try:\n",
    "                shutil.move(source_json_path, dest_json_path)\n",
    "                # print(f\"Moved orphaned JSON: '{json_filename}' to '{orphaned_dest_directory}'\")\n",
    "                moved_count += 1\n",
    "            except FileNotFoundError:\n",
    "                 print(f\"Warning: JSON file '{json_filename}' was listed but not found during move operation. It might have been moved or deleted by another process.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error moving JSON file '{json_filename}': {e}\")\n",
    "        \n",
    "    print(f\"\\n--- Orphaned JSON Scan Complete (Optimized) ---\")\n",
    "    print(f\"Total JSON files checked: {len(json_files_to_process)}.\")\n",
    "    print(f\"Identified {orphaned_json_found_count} as potentially orphaned.\")\n",
    "    print(f\"Successfully moved {moved_count} orphaned JSON files to '{orphaned_dest_directory}'.\")\n",
    "\n",
    "# --- Main execution ---\n",
    "if __name__ == \"__main__\": # Good practice for notebooks too\n",
    "    # Ensure SOURCE_DIR and ORPHANED_JSON_DEST_DIR are correctly set\n",
    "    if not SOURCE_DIR.startswith(\"path/to/your\") and \\\n",
    "       not ORPHANED_JSON_DEST_DIR.startswith(\"path/to/your\"):\n",
    "        move_orphaned_json_files_optimized(SOURCE_DIR, ORPHANED_JSON_DEST_DIR, IMAGE_EXTENSIONS)\n",
    "    else:\n",
    "        print(\"ERROR: Please update SOURCE_DIR and ORPHANED_JSON_DEST_DIR variables in the script.\")\n",
    "        print(\"SOURCE_DIR is currently:\", SOURCE_DIR)\n",
    "        print(\"ORPHANED_JSON_DEST_DIR is currently:\", ORPHANED_JSON_DEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad064d22-a67d-45a3-8f57-cd18674e58f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b0719-cb27-4508-a001-aad37eec3bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5902f9ae-2c3f-4781-a7f4-8801aa494c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e651d-4011-493a-a224-d08d2066db70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a005f1-0904-49fd-8f94-aa436b78e0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8352a-fcd8-4bd2-8464-fbf775e4c630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f2962-d1f3-4808-93e5-3124bc1a257f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e4190a-c616-43dd-9c41-ed7ebe6f1ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a3013-1fe0-4d24-9cc6-890eecb03637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b052ed2-7ef8-4a9f-9191-bfaf380ebeb3",
   "metadata": {},
   "source": [
    "### Copy files fast with the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eed98a7-a360-4117-b1e6-9fa2dd4c5545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import multiprocessing\n",
    "# import time # argparse is removed\n",
    "\n",
    "# def copy_file_worker(src_path, dst_path):\n",
    "#     \"\"\"\n",
    "#     Worker function to copy a single file.\n",
    "#     This function is intended to be executed by a process in a multiprocessing pool.\n",
    "\n",
    "#     Args:\n",
    "#         src_path (str): Path to the source file.\n",
    "#         dst_path (str): Path to the destination file where the source file will be copied.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing:\n",
    "#             - src_path (str): The path of the source file processed.\n",
    "#             - success_status (bool): True if copying was successful, False otherwise.\n",
    "#             - error_message (str or None): An error message string if copying failed, None otherwise.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Ensure the destination directory for the file exists.\n",
    "#         os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n",
    "        \n",
    "#         # shutil.copy2 attempts to preserve all metadata (like timestamps, permissions)\n",
    "#         shutil.copy2(src_path, dst_path)\n",
    "        \n",
    "#         # print(f\"Copied: {src_path} -> {dst_path}\") # Uncomment for verbose output\n",
    "#         return src_path, True, None\n",
    "#     except Exception as e:\n",
    "#         # print(f\"Error copying {src_path} to {dst_path}: {e}\") # Uncomment for immediate error feedback\n",
    "#         return src_path, False, str(e)\n",
    "\n",
    "# def parallel_copy_folder(source_folder, destination_folder, num_processes=None):\n",
    "#     \"\"\"\n",
    "#     Copies a folder from a source path to a destination path in parallel.\n",
    "#     It recreates the directory structure from the source in the destination\n",
    "#     and then copies files using multiple processes.\n",
    "\n",
    "#     Args:\n",
    "#         source_folder (str): The path to the source folder to be copied.\n",
    "#         destination_folder (str): The path to the destination folder. If it doesn't exist,\n",
    "#                                   it will be created. If it exists, files will be copied into it.\n",
    "#         num_processes (int, optional): The number of worker processes to use for copying files.\n",
    "#                                        Defaults to the number of CPU cores available (os.cpu_count()).\n",
    "#     \"\"\"\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # --- 1. Validate Source Folder ---\n",
    "#     if not os.path.exists(source_folder):\n",
    "#         print(f\"Error: Source folder '{source_folder}' does not exist.\")\n",
    "#         return\n",
    "#     if not os.path.isdir(source_folder):\n",
    "#         print(f\"Error: Source '{source_folder}' is not a directory.\")\n",
    "#         return\n",
    "\n",
    "#     # --- 2. Prepare Destination Folder ---\n",
    "#     source_folder_abs = os.path.abspath(source_folder)\n",
    "#     destination_folder_abs = os.path.abspath(destination_folder)\n",
    "\n",
    "#     if os.path.exists(destination_folder_abs):\n",
    "#         if not os.path.isdir(destination_folder_abs):\n",
    "#             print(f\"Error: Destination '{destination_folder_abs}' exists and is not a directory.\")\n",
    "#             return\n",
    "#         # print(f\"Warning: Destination folder '{destination_folder_abs}' already exists. Files may be overwritten.\")\n",
    "#     else:\n",
    "#         try:\n",
    "#             os.makedirs(destination_folder_abs, exist_ok=True)\n",
    "#             print(f\"Created destination folder: {destination_folder_abs}\")\n",
    "#         except OSError as e:\n",
    "#             print(f\"Error creating destination folder '{destination_folder_abs}': {e}\")\n",
    "#             return\n",
    "\n",
    "#     # --- 3. Collect File Copy Tasks and Create Directory Structure ---\n",
    "#     tasks = [] \n",
    "\n",
    "#     print(f\"Scanning source folder: {source_folder_abs}...\")\n",
    "#     for dirpath, dirnames, filenames in os.walk(source_folder_abs):\n",
    "#         relative_path = os.path.relpath(dirpath, source_folder_abs)\n",
    "        \n",
    "#         if relative_path == \".\":\n",
    "#             dest_dirpath = destination_folder_abs\n",
    "#         else:\n",
    "#             dest_dirpath = os.path.join(destination_folder_abs, relative_path)\n",
    "\n",
    "#         if not os.path.exists(dest_dirpath):\n",
    "#             try:\n",
    "#                 os.makedirs(dest_dirpath, exist_ok=True)\n",
    "#             except OSError as e:\n",
    "#                 print(f\"Error creating directory {dest_dirpath}: {e}\")\n",
    "#                 continue \n",
    "\n",
    "#         for filename in filenames:\n",
    "#             src_file_path = os.path.join(dirpath, filename)\n",
    "#             dst_file_path = os.path.join(dest_dirpath, filename)\n",
    "#             tasks.append((src_file_path, dst_file_path))\n",
    "\n",
    "#     if not tasks:\n",
    "#         print(\"No files found to copy in the source folder.\")\n",
    "#         duration = time.time() - start_time\n",
    "#         print(f\"Operation completed in {duration:.2f} seconds (no files to copy).\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Found {len(tasks)} files to copy. Starting parallel copy operation...\")\n",
    "\n",
    "#     # --- 4. Execute Copy Tasks in Parallel ---\n",
    "#     if num_processes is None:\n",
    "#         num_processes = os.cpu_count() \n",
    "    \n",
    "#     actual_num_processes = min(num_processes, len(tasks)) \n",
    "    \n",
    "#     print(f\"Using {actual_num_processes} processes for copying.\")\n",
    "\n",
    "#     results = []\n",
    "#     # Important: On some systems, especially Windows, multiprocessing within a Jupyter Notebook\n",
    "#     # might require the worker function and the pool execution to be guarded by `if __name__ == '__main__':`.\n",
    "#     # However, for a single cell execution, this structure is generally fine.\n",
    "#     # If you encounter issues with multiprocessing (e.g., hanging),\n",
    "#     # you might need to move the worker function definition and the pool logic\n",
    "#     # into a separate .py file and import it, or investigate platform-specific\n",
    "#     # multiprocessing behavior in notebooks.\n",
    "#     with multiprocessing.Pool(processes=actual_num_processes) as pool:\n",
    "#         results = pool.starmap(copy_file_worker, tasks)\n",
    "\n",
    "#     # --- 5. Report Results ---\n",
    "#     successful_copies = 0\n",
    "#     failed_copies_details = [] \n",
    "\n",
    "#     for src_path, success, error_msg in results:\n",
    "#         if success:\n",
    "#             successful_copies += 1\n",
    "#         else:\n",
    "#             failed_copies_details.append((src_path, error_msg))\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     duration = end_time - start_time\n",
    "\n",
    "#     print(f\"\\n--- Copy Operation Summary ---\")\n",
    "#     print(f\"Total files scheduled for copy: {len(tasks)}\")\n",
    "#     print(f\"Successfully copied files: {successful_copies}\")\n",
    "#     print(f\"Failed to copy files: {len(failed_copies_details)}\")\n",
    "    \n",
    "#     if failed_copies_details:\n",
    "#         print(\"\\nDetails of failed copies:\")\n",
    "#         for src, err in failed_copies_details:\n",
    "#             print(f\"  - File: {src}\\n    Error: {err}\")\n",
    "            \n",
    "#     print(f\"\\nTotal time taken for copy operation: {duration:.2f} seconds.\")\n",
    "\n",
    "# # --- Configuration for Jupyter Notebook ---\n",
    "# # IMPORTANT: Define your source, destination, and number of processes here.\n",
    "# # Make sure to use raw strings (r\"...\") or escape backslashes (\\) for Windows paths.\n",
    "\n",
    "# # Example for Linux/macOS:\n",
    "# # MY_SOURCE_FOLDER = \"/path/to/your/source_folder\"\n",
    "# # MY_DESTINATION_FOLDER = \"/path/to/your/destination_folder\"\n",
    "\n",
    "# # Example for Windows:\n",
    "# # MY_SOURCE_FOLDER = r\"C:\\path\\to\\your\\source_folder\"\n",
    "# # MY_DESTINATION_FOLDER = r\"C:\\path\\to\\your\\destination_folder\"\n",
    "\n",
    "# # Fill these in with your actual paths:\n",
    "# MY_SOURCE_FOLDER = r\"/blue/hulcr/gmarais/PhD/phase_1_data/20251505/output\"  # e.g., r\"/mnt/data/source_dir\" or r\"D:\\source_data\"\n",
    "# MY_DESTINATION_FOLDER = r\"/blue/hulcr/gmarais/data_backups/with_duplicates_20250515\" # e.g., r\"/mnt/data/destination_dir\" or r\"E:\\backup_data\"\n",
    "# NUMBER_OF_PROCESSES = None  # Set to an integer (e.g., 4) or None to use all available CPU cores\n",
    "\n",
    "# # --- Run the copy operation ---\n",
    "# if MY_SOURCE_FOLDER == \"REPLACE_WITH_YOUR_SOURCE_PATH\" or \\\n",
    "#    MY_DESTINATION_FOLDER == \"REPLACE_WITH_YOUR_DESTINATION_PATH\":\n",
    "#     print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "#     print(\"!!! PLEASE UPDATE 'MY_SOURCE_FOLDER' AND 'MY_DESTINATION_FOLDER'          !!!\")\n",
    "#     print(\"!!! in the script before running to specify your actual paths.            !!!\")\n",
    "#     print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "# else:\n",
    "#     print(f\"Initiating folder copy from '{MY_SOURCE_FOLDER}' to '{MY_DESTINATION_FOLDER}'...\")\n",
    "#     if NUMBER_OF_PROCESSES:\n",
    "#         print(f\"User specified number of processes: {NUMBER_OF_PROCESSES}\")\n",
    "#     else:\n",
    "#         print(f\"Number of processes will default to available CPU cores.\")\n",
    "\n",
    "#     # Call the main function to perform the copy\n",
    "#     # This part is no longer inside an `if __name__ == \"__main__\":` block\n",
    "#     # as it's intended to be run directly in a notebook cell.\n",
    "#     parallel_copy_folder(MY_SOURCE_FOLDER, MY_DESTINATION_FOLDER, NUMBER_OF_PROCESSES)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
