{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a91566-4783-4fb1-9c6f-4bfb6331ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "# import yaml # Not strictly needed for COCO JSON output, but was in previous script\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# --- COCO Bounding Box Conversion ---\n",
    "def convert_to_coco_bbox(points, image_height, image_width):\n",
    "    \"\"\"\n",
    "    Converts [[x1, y1], [x2, y2]] points to COCO bbox [x_min, y_min, width, height].\n",
    "    Clamps coordinates to be within image boundaries.\n",
    "    \"\"\"\n",
    "    x_coords = [p[0] for p in points]\n",
    "    y_coords = [p[1] for p in points]\n",
    "\n",
    "    x_min_abs = float(min(x_coords))\n",
    "    y_min_abs = float(min(y_coords))\n",
    "    x_max_abs = float(max(x_coords))\n",
    "    y_max_abs = float(max(y_coords))\n",
    "\n",
    "    x_min_abs = max(0.0, x_min_abs)\n",
    "    y_min_abs = max(0.0, y_min_abs)\n",
    "    x_max_abs = min(float(image_width - 1), x_max_abs)\n",
    "    y_max_abs = min(float(image_height - 1), y_max_abs)\n",
    "    \n",
    "    if x_min_abs >= x_max_abs or y_min_abs >= y_max_abs:\n",
    "        return None\n",
    "\n",
    "    width = x_max_abs - x_min_abs\n",
    "    height = y_max_abs - y_min_abs\n",
    "    \n",
    "    return [x_min_abs, y_min_abs, width, height]\n",
    "\n",
    "# --- Fixed Category Definition for COCO ---\n",
    "def get_fixed_coco_category_list(class_name=\"bark_beetle\", class_id=1):\n",
    "    \"\"\"Returns a COCO categories list for a single, fixed class.\"\"\"\n",
    "    print(f\"Defining fixed COCO category: ID: {class_id}, Name: {class_name}\")\n",
    "    return [{\n",
    "        \"id\": class_id,\n",
    "        \"name\": class_name,\n",
    "        \"supercategory\": class_name # Or a more general one if applicable\n",
    "    }]\n",
    "\n",
    "# --- Worker for processing one image-JSON pair for COCO (Single Class) ---\n",
    "def _process_single_image_to_coco_data_single_class(args_tuple):\n",
    "    \"\"\"\n",
    "    Processes one image and its JSON. All shapes are mapped to a single class name.\n",
    "    Returns dict with image_info and list of annotation_data (pre-ID assignment).\n",
    "    args_tuple: (img_filename_no_ext, source_fold_path, new_img_filename, fixed_class_name)\n",
    "    \"\"\"\n",
    "    img_filename_no_ext, source_fold_path, new_img_filename, fixed_class_name = args_tuple\n",
    "    \n",
    "    png_file = f\"{img_filename_no_ext}.png\"\n",
    "    json_file = f\"{img_filename_no_ext}.json\"\n",
    "\n",
    "    source_png_path = os.path.join(source_fold_path, png_file)\n",
    "    source_json_path = os.path.join(source_fold_path, json_file)\n",
    "\n",
    "    if not (os.path.exists(source_png_path) and os.path.exists(source_json_path)):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(source_json_path, 'r') as f:\n",
    "            user_json_data = json.load(f)\n",
    "        \n",
    "        image_height = user_json_data.get(\"imageHeight\")\n",
    "        image_width = user_json_data.get(\"imageWidth\")\n",
    "\n",
    "        if image_height is None or image_width is None:\n",
    "            with Image.open(source_png_path) as img_pil:\n",
    "                pil_width, pil_height = img_pil.size\n",
    "            if image_width is None: image_width = pil_width\n",
    "            if image_height is None: image_height = pil_height\n",
    "        \n",
    "        if not image_height or not image_width:\n",
    "            return None\n",
    "\n",
    "        image_info = {\n",
    "            \"file_name\": new_img_filename,\n",
    "            \"height\": int(image_height),\n",
    "            \"width\": int(image_width),\n",
    "            \"original_path\": source_png_path\n",
    "        }\n",
    "        \n",
    "        annotations_data = []\n",
    "        for shape in user_json_data.get(\"shapes\", []):\n",
    "            points = shape.get(\"points\")\n",
    "            shape_type = shape.get(\"shape_type\")\n",
    "\n",
    "            if not points or shape_type != \"rectangle\" or len(points) != 2:\n",
    "                continue\n",
    "            \n",
    "            coco_bbox = convert_to_coco_bbox(points, image_height, image_width)\n",
    "            if coco_bbox:\n",
    "                annotations_data.append({\n",
    "                    \"category_name\": fixed_class_name, # Use the fixed class name\n",
    "                    \"bbox\": coco_bbox,\n",
    "                    \"area\": coco_bbox[2] * coco_bbox[3]\n",
    "                })\n",
    "        \n",
    "        if not annotations_data and not image_info.get('force_include_empty', False):\n",
    "             if not image_info: return None\n",
    "        \n",
    "        return {\"image_info\": image_info, \"annotations_data\": annotations_data, \"original_source_path\": source_png_path}\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Warning (COCO Worker - Single Class): Error processing {source_png_path} or {source_json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Orchestrator Function for Jupyter Notebook (Single Class COCO) ---\n",
    "def create_kfold_coco_datasets_single_class(source_dir, base_dest_dir, source_fold_names_str,\n",
    "                                            fixed_class_name=\"bark_beetle\", fixed_class_id=1,\n",
    "                                            max_workers=None):\n",
    "    \"\"\"\n",
    "    Main function to create k-fold cross-validation datasets in COCO format\n",
    "    with all objects mapped to a single class.\n",
    "    \"\"\"\n",
    "    overall_start_time = time.time()\n",
    "    all_original_fold_names = [name.strip() for name in source_fold_names_str.split(',') if name.strip()]\n",
    "\n",
    "    if not all_original_fold_names or len(all_original_fold_names) < 2:\n",
    "        print(\"Error: Please provide at least two source fold names for cross-validation.\")\n",
    "        return\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "        return\n",
    "    os.makedirs(base_dest_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Get the fixed COCO category list\n",
    "    coco_categories = get_fixed_coco_category_list(class_name=fixed_class_name, class_id=fixed_class_id)\n",
    "    # This will be a list with one category object.\n",
    "    # The category_name_to_id map will simply be {fixed_class_name: fixed_class_id}\n",
    "    category_name_to_id = {cat['name']: cat['id'] for cat in coco_categories}\n",
    "\n",
    "\n",
    "    num_cv_folds = len(all_original_fold_names)\n",
    "    print(f\"\\nPreparing data for {num_cv_folds}-Fold Cross-Validation (COCO Format, Single Class: '{fixed_class_name}')...\")\n",
    "\n",
    "    for i in range(num_cv_folds): # Loop for each CV iteration\n",
    "        cv_iteration_start_time = time.time()\n",
    "        current_val_fold_name = all_original_fold_names[i]\n",
    "        current_train_fold_names = [f_name for idx, f_name in enumerate(all_original_fold_names) if idx != i]\n",
    "\n",
    "        cv_iteration_dir_name = f\"cv_iteration_{i+1}\"\n",
    "        current_cv_split_output_root = os.path.join(base_dest_dir, cv_iteration_dir_name)\n",
    "        print(f\"\\n--- Processing CV Iteration {i+1}/{num_cv_folds} ---\")\n",
    "        print(f\"  Output to: {current_cv_split_output_root}\")\n",
    "\n",
    "        train_img_dir = os.path.join(current_cv_split_output_root, \"train\")\n",
    "        val_img_dir = os.path.join(current_cv_split_output_root, \"val\")\n",
    "        annotations_dir = os.path.join(current_cv_split_output_root, \"annotations\")\n",
    "        os.makedirs(train_img_dir, exist_ok=True)\n",
    "        os.makedirs(val_img_dir, exist_ok=True)\n",
    "        os.makedirs(annotations_dir, exist_ok=True)\n",
    "\n",
    "        for split_type, source_fold_list in [(\"train\", current_train_fold_names), (\"val\", [current_val_fold_name])]:\n",
    "            print(f\"  Processing {split_type} data for CV Iteration {i+1}...\")\n",
    "            split_start_time = time.time()\n",
    "\n",
    "            coco_output_data = {\n",
    "                \"info\": {\n",
    "                    \"description\": f\"COCO-style dataset for CV Iteration {i+1} - {split_type} (Single Class: {fixed_class_name})\",\n",
    "                    \"version\": \"1.0\", \"year\": datetime.date.today().year,\n",
    "                    \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
    "                },\n",
    "                \"licenses\": [{\"name\": \"Placeholder License\", \"id\": 0, \"url\": \"\"}],\n",
    "                \"categories\": coco_categories, # Use the fixed list\n",
    "                \"images\": [],\n",
    "                \"annotations\": []\n",
    "            }\n",
    "            \n",
    "            current_image_id = 1\n",
    "            current_annotation_id = 1\n",
    "            tasks_for_split = []\n",
    "            target_image_dir_for_split = train_img_dir if split_type == \"train\" else val_img_dir\n",
    "\n",
    "            for fold_idx, fold_name in enumerate(source_fold_list):\n",
    "                source_fold_path = os.path.join(source_dir, fold_name)\n",
    "                if not os.path.isdir(source_fold_path):\n",
    "                    print(f\"    Warning: Source fold '{source_fold_path}' for {split_type} not found. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                image_basenames = sorted([os.path.splitext(f)[0] for f in os.listdir(source_fold_path) if f.lower().endswith(\".png\")])\n",
    "                for img_basename in image_basenames:\n",
    "                    new_img_filename = f\"{split_type}_fold{fold_idx}_{img_basename}.png\"\n",
    "                    # Pass fixed_class_name to the worker\n",
    "                    tasks_for_split.append((img_basename, source_fold_path, new_img_filename, fixed_class_name)) \n",
    "            \n",
    "            if not tasks_for_split:\n",
    "                print(f\"    No images found to process for {split_type} set in this CV iteration.\")\n",
    "            else:\n",
    "                processed_results = []\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    # Use the modified worker function\n",
    "                    future_to_task = {executor.submit(_process_single_image_to_coco_data_single_class, task_args): task_args for task_args in tasks_for_split}\n",
    "                    for future in concurrent.futures.as_completed(future_to_task):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                processed_results.append(result)\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                \n",
    "                for result_data in processed_results:\n",
    "                    shutil.copy2(result_data[\"original_source_path\"], os.path.join(target_image_dir_for_split, result_data[\"image_info\"][\"file_name\"]))\n",
    "                    img_entry = result_data[\"image_info\"]\n",
    "                    img_entry[\"id\"] = current_image_id \n",
    "                    del img_entry[\"original_path\"] \n",
    "                    coco_output_data[\"images\"].append(img_entry) \n",
    "                    \n",
    "                    for ann_data in result_data[\"annotations_data\"]:\n",
    "                        # ann_data[\"category_name\"] will already be fixed_class_name\n",
    "                        # Ensure it's in our simple category_name_to_id map\n",
    "                        if ann_data[\"category_name\"] not in category_name_to_id:\n",
    "                             print(f\"    Logic Error: Fixed class name '{ann_data['category_name']}' not in map. This should not happen.\")\n",
    "                             continue\n",
    "\n",
    "                        ann_entry = {\n",
    "                            \"id\": current_annotation_id,\n",
    "                            \"image_id\": current_image_id,\n",
    "                            \"category_id\": category_name_to_id[ann_data[\"category_name\"]], # Should be fixed_class_id\n",
    "                            \"bbox\": ann_data[\"bbox\"],\n",
    "                            \"area\": ann_data[\"area\"],\n",
    "                            \"iscrowd\": 0,\n",
    "                            \"segmentation\": [] \n",
    "                        }\n",
    "                        coco_output_data[\"annotations\"].append(ann_entry)\n",
    "                        current_annotation_id += 1\n",
    "                    current_image_id += 1\n",
    "            \n",
    "            output_json_filename = f\"instances_{split_type}.json\"\n",
    "            output_json_path = os.path.join(annotations_dir, output_json_filename)\n",
    "            try:\n",
    "                with open(output_json_path, 'w') as f:\n",
    "                    json.dump(coco_output_data, f, indent=4)\n",
    "                split_processing_time = time.time() - split_start_time\n",
    "                print(f\"    Successfully created '{output_json_filename}' with {len(coco_output_data['images'])} images and {len(coco_output_data['annotations'])} annotations in {split_processing_time:.2f}s.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error writing COCO JSON '{output_json_filename}': {e}\")\n",
    "        \n",
    "        cv_iteration_time = time.time() - cv_iteration_start_time\n",
    "        print(f\"  CV Iteration {i+1} processing took {cv_iteration_time:.2f}s.\")\n",
    "\n",
    "    total_script_time = time.time() - overall_start_time\n",
    "    print(f\"\\n{num_cv_folds}-Fold Cross-Validation COCO dataset (Single Class: '{fixed_class_name}') preparation complete in {total_script_time:.2f} seconds!\")\n",
    "    print(f\"All CV iteration datasets are ready under: {base_dest_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ea5042-1699-4fa7-9bce-3fb698d6dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting COCO dataset creation for Co-DETR with K-Fold CV (Single Class: 'bark_beetle'):\n",
      "  Source Directory: /blue/hulcr/gmarais/PhD/phase_1_data/1_data_splitting/test_set_output\n",
      "  Base Destination Directory: /blue/hulcr/gmarais/PhD/phase_1_data/2_object_detection_phase_2/coco/test\n",
      "  Source Fold Names: ['/']\n",
      "  Max Workers for Parallelization: Default (likely 128)\n",
      "------------------------------\n",
      "Error: Please provide at least two source fold names for cross-validation.\n",
      "\n",
      "COCO Single-Class Script execution finished in this cell.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for Your Dataset (COCO Format - Single Class) ---\n",
    "\n",
    "# **IMPORTANT**: Modify these paths and names to match your actual dataset and desired output.\n",
    "SOURCE_DIR = \"/blue/hulcr/gmarais/PhD/phase_1_data/1_data_splitting\"\n",
    "BASE_DEST_DIR = \"/blue/hulcr/gmarais/PhD/phase_1_data/2_object_detection_phase_2/coco/test\"\n",
    "SOURCE_FOLD_NAMES_STR = \"test_set_output\" # Comma-separated\n",
    "\n",
    "# Define the single class name and its ID (COCO typically starts IDs at 1)\n",
    "FIXED_CLASS_NAME = \"bark_beetle\"\n",
    "FIXED_CLASS_ID = 1 # Make sure this ID is used consistently if your model expects a specific ID\n",
    "\n",
    "# Optional: Control the number of parallel processes\n",
    "import os # if you want to use os.cpu_count() explicitly\n",
    "MAX_WORKERS = None # Defaults to os.cpu_count()\n",
    "# MAX_WORKERS = 4 # Or set a specific number\n",
    "\n",
    "# --- Run the Dataset Creation ---\n",
    "print(f\"Starting COCO dataset creation for Co-DETR with K-Fold CV (Single Class: '{FIXED_CLASS_NAME}'):\")\n",
    "print(f\"  Source Directory: {SOURCE_DIR}\")\n",
    "print(f\"  Base Destination Directory: {BASE_DEST_DIR}\")\n",
    "print(f\"  Source Fold Names: {SOURCE_FOLD_NAMES_STR.split(',')}\")\n",
    "print(f\"  Max Workers for Parallelization: {MAX_WORKERS if MAX_WORKERS is not None else f'Default (likely {os.cpu_count()})'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ensure the function create_kfold_coco_datasets_single_class is defined by running the cell above first!\n",
    "create_kfold_coco_datasets_single_class(\n",
    "    source_dir=SOURCE_DIR,\n",
    "    base_dest_dir=BASE_DEST_DIR,\n",
    "    source_fold_names_str=SOURCE_FOLD_NAMES_STR,\n",
    "    fixed_class_name=FIXED_CLASS_NAME,\n",
    "    fixed_class_id=FIXED_CLASS_ID,\n",
    "    max_workers=MAX_WORKERS\n",
    ")\n",
    "\n",
    "print(\"\\nCOCO Single-Class Script execution finished in this cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
