{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e77db40-f09c-471a-8172-0fa4f811dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "import yaml # Still useful for reading/writing other configs if needed, but mainly json here\n",
    "from PIL import Image\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# --- COCO Bounding Box Conversion ---\n",
    "def convert_to_coco_bbox(points, image_height, image_width):\n",
    "    \"\"\"\n",
    "    Converts [[x1, y1], [x2, y2]] points to COCO bbox [x_min, y_min, width, height].\n",
    "    Clamps coordinates to be within image boundaries.\n",
    "    \"\"\"\n",
    "    x_coords = [p[0] for p in points]\n",
    "    y_coords = [p[1] for p in points]\n",
    "\n",
    "    x_min_abs = float(min(x_coords))\n",
    "    y_min_abs = float(min(y_coords))\n",
    "    x_max_abs = float(max(x_coords))\n",
    "    y_max_abs = float(max(y_coords))\n",
    "\n",
    "    # Clamp to image boundaries\n",
    "    x_min_abs = max(0.0, x_min_abs)\n",
    "    y_min_abs = max(0.0, y_min_abs)\n",
    "    x_max_abs = min(float(image_width - 1), x_max_abs)\n",
    "    y_max_abs = min(float(image_height - 1), y_max_abs)\n",
    "    \n",
    "    if x_min_abs >= x_max_abs or y_min_abs >= y_max_abs:\n",
    "        # print(f\"Warning: Invalid bbox after clamping: {[x_min_abs, y_min_abs, x_max_abs - x_min_abs, y_max_abs - y_min_abs]}\")\n",
    "        return None\n",
    "\n",
    "    width = x_max_abs - x_min_abs\n",
    "    height = y_max_abs - y_min_abs\n",
    "    \n",
    "    return [x_min_abs, y_min_abs, width, height]\n",
    "\n",
    "# --- Parallelized Category Discovery for COCO ---\n",
    "def _get_labels_from_single_json_for_coco(json_path):\n",
    "    \"\"\"Helper function to extract labels from a single JSON file for COCO categories.\"\"\"\n",
    "    labels_in_file = set()\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        for shape in data.get(\"shapes\", []): # Assuming 'shapes' contains the objects\n",
    "            label = shape.get(\"label\")\n",
    "            if label:\n",
    "                labels_in_file.add(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning (Category Discovery Worker): Error reading {json_path}: {e}\")\n",
    "    return labels_in_file\n",
    "\n",
    "def discover_coco_categories_parallel(source_root_dir, all_fold_names, max_workers=None):\n",
    "    \"\"\"Scans all specified folds in parallel to discover unique class labels for COCO categories.\"\"\"\n",
    "    print(\"Discovering COCO categories across all specified folds (parallelized)...\")\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    json_file_paths = []\n",
    "    for fold_name in all_fold_names:\n",
    "        current_fold_path = os.path.join(source_root_dir, fold_name)\n",
    "        if not os.path.isdir(current_fold_path):\n",
    "            print(f\"Warning (Category Discovery): Source fold '{current_fold_path}' not found. Skipping.\")\n",
    "            continue\n",
    "        for item_name in os.listdir(current_fold_path):\n",
    "            if item_name.lower().endswith(\".json\"): # Assuming .json files contain labels\n",
    "                json_file_paths.append(os.path.join(current_fold_path, item_name))\n",
    "\n",
    "    if not json_file_paths:\n",
    "        print(\"Warning: No JSON files found for category discovery.\")\n",
    "        return [] # Return empty list if no categories found\n",
    "\n",
    "    unique_labels = set()\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_path = {executor.submit(_get_labels_from_single_json_for_coco, path): path for path in json_file_paths}\n",
    "        for future in concurrent.futures.as_completed(future_to_path):\n",
    "            try:\n",
    "                labels_from_file = future.result()\n",
    "                unique_labels.update(labels_from_file)\n",
    "            except Exception as exc:\n",
    "                path = future_to_path[future]\n",
    "                print(f\"Warning (Category Discovery Main): Generated an exception for {path}: {exc}\")\n",
    "    \n",
    "    sorted_labels = sorted(list(unique_labels))\n",
    "    \n",
    "    categories_list = []\n",
    "    # COCO category IDs typically start from 1. Some models might handle 0 as background.\n",
    "    # Let's start from 1 for distinct objects.\n",
    "    for i, label_name in enumerate(sorted_labels):\n",
    "        categories_list.append({\n",
    "            \"id\": i + 1, # Category ID\n",
    "            \"name\": label_name,\n",
    "            \"supercategory\": label_name # Or a more general supercategory if you have one\n",
    "        })\n",
    "    \n",
    "    discovery_time = time.time() - overall_start_time\n",
    "    if categories_list:\n",
    "        print(f\"Discovered {len(categories_list)} COCO categories in {discovery_time:.2f} seconds:\")\n",
    "        for cat in categories_list:\n",
    "            print(f\"  ID: {cat['id']}, Name: {cat['name']}\")\n",
    "    else:\n",
    "        print(f\"Warning: No categories discovered after parallel processing in {discovery_time:.2f} seconds.\")\n",
    "    return categories_list\n",
    "\n",
    "\n",
    "# --- Worker for processing one image-JSON pair for COCO ---\n",
    "def _process_single_image_to_coco_data(args_tuple):\n",
    "    \"\"\"\n",
    "    Processes one image and its JSON.\n",
    "    Returns dict with image_info and list of annotation_data (pre-ID assignment).\n",
    "    args_tuple: (img_filename_no_ext, source_fold_path, new_img_filename)\n",
    "    \"\"\"\n",
    "    img_filename_no_ext, source_fold_path, new_img_filename = args_tuple\n",
    "    \n",
    "    png_file = f\"{img_filename_no_ext}.png\"\n",
    "    json_file = f\"{img_filename_no_ext}.json\" # Assuming same name for JSON\n",
    "\n",
    "    source_png_path = os.path.join(source_fold_path, png_file)\n",
    "    source_json_path = os.path.join(source_fold_path, json_file)\n",
    "\n",
    "    if not (os.path.exists(source_png_path) and os.path.exists(source_json_path)):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(source_json_path, 'r') as f:\n",
    "            user_json_data = json.load(f)\n",
    "        \n",
    "        image_height = user_json_data.get(\"imageHeight\")\n",
    "        image_width = user_json_data.get(\"imageWidth\")\n",
    "\n",
    "        # Fallback if dimensions not in JSON\n",
    "        if image_height is None or image_width is None:\n",
    "            with Image.open(source_png_path) as img_pil:\n",
    "                pil_width, pil_height = img_pil.size\n",
    "            if image_width is None: image_width = pil_width\n",
    "            if image_height is None: image_height = pil_height\n",
    "        \n",
    "        if not image_height or not image_width: # Handles 0 or None\n",
    "            return None\n",
    "\n",
    "        image_info = {\n",
    "            \"file_name\": new_img_filename, # This will be relative to train/ or val/\n",
    "            \"height\": int(image_height),\n",
    "            \"width\": int(image_width),\n",
    "            \"original_path\": source_png_path # For copying later\n",
    "        }\n",
    "        \n",
    "        annotations_data = []\n",
    "        for shape in user_json_data.get(\"shapes\", []): # Assuming 'shapes' from user's JSON\n",
    "            original_label = shape.get(\"label\")\n",
    "            points = shape.get(\"points\") # Assuming format [[x1,y1],[x2,y2]]\n",
    "            shape_type = shape.get(\"shape_type\")\n",
    "\n",
    "            if not original_label or not points or shape_type != \"rectangle\" or len(points) != 2:\n",
    "                continue\n",
    "            \n",
    "            coco_bbox = convert_to_coco_bbox(points, image_height, image_width)\n",
    "            if coco_bbox:\n",
    "                annotations_data.append({\n",
    "                    \"category_name\": original_label, # Will be mapped to category_id later\n",
    "                    \"bbox\": coco_bbox,\n",
    "                    \"area\": coco_bbox[2] * coco_bbox[3]\n",
    "                })\n",
    "        \n",
    "        if not annotations_data and not image_info.get('force_include_empty', False): # Only include if there are annotations\n",
    "            # Or decide if you want to include images with no annotations\n",
    "            # For COCO, usually images listed have annotations, but it's not strictly enforced by format\n",
    "            # For now, let's only return data if there are annotations, or image_info is present.\n",
    "             if not image_info: return None # No image info, def skip\n",
    "\n",
    "        return {\"image_info\": image_info, \"annotations_data\": annotations_data, \"original_source_path\": source_png_path}\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Warning (COCO Worker): Error processing {source_png_path} or {source_json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Orchestrator Function for Jupyter Notebook ---\n",
    "def create_kfold_coco_datasets(source_dir, base_dest_dir, source_fold_names_str, max_workers=None):\n",
    "    \"\"\"\n",
    "    Main function to create k-fold cross-validation datasets in COCO format.\n",
    "    \"\"\"\n",
    "    overall_start_time = time.time()\n",
    "    all_original_fold_names = [name.strip() for name in source_fold_names_str.split(',') if name.strip()]\n",
    "\n",
    "    if not all_original_fold_names or len(all_original_fold_names) < 2:\n",
    "        print(\"Error: Please provide at least two source fold names for cross-validation.\")\n",
    "        return\n",
    "    if not os.path.isdir(source_dir):\n",
    "        print(f\"Error: Source directory '{source_dir}' not found.\")\n",
    "        return\n",
    "    os.makedirs(base_dest_dir, exist_ok=True)\n",
    "\n",
    "    # 1. Discover all COCO categories globally first\n",
    "    coco_categories = discover_coco_categories_parallel(source_dir, all_original_fold_names, max_workers)\n",
    "    if not coco_categories:\n",
    "        print(\"Error: No categories found. Cannot proceed.\")\n",
    "        return\n",
    "    \n",
    "    # Create a quick lookup map from category name to category ID\n",
    "    category_name_to_id = {cat['name']: cat['id'] for cat in coco_categories}\n",
    "\n",
    "    num_cv_folds = len(all_original_fold_names)\n",
    "    print(f\"\\nPreparing data for {num_cv_folds}-Fold Cross-Validation (COCO Format)...\")\n",
    "\n",
    "    for i in range(num_cv_folds): # Loop for each CV iteration\n",
    "        cv_iteration_start_time = time.time()\n",
    "        current_val_fold_name = all_original_fold_names[i]\n",
    "        current_train_fold_names = [f_name for idx, f_name in enumerate(all_original_fold_names) if idx != i]\n",
    "\n",
    "        cv_iteration_dir_name = f\"cv_iteration_{i+1}\"\n",
    "        current_cv_split_output_root = os.path.join(base_dest_dir, cv_iteration_dir_name)\n",
    "        print(f\"\\n--- Processing CV Iteration {i+1}/{num_cv_folds} ---\")\n",
    "        print(f\"  Output to: {current_cv_split_output_root}\")\n",
    "\n",
    "        # Define paths for this CV iteration\n",
    "        train_img_dir = os.path.join(current_cv_split_output_root, \"train\") # Corresponds to e.g. train2017\n",
    "        val_img_dir = os.path.join(current_cv_split_output_root, \"val\")     # Corresponds to e.g. val2017\n",
    "        annotations_dir = os.path.join(current_cv_split_output_root, \"annotations\")\n",
    "        os.makedirs(train_img_dir, exist_ok=True)\n",
    "        os.makedirs(val_img_dir, exist_ok=True)\n",
    "        os.makedirs(annotations_dir, exist_ok=True)\n",
    "\n",
    "        # Process TRAIN and VAL splits for the current CV iteration\n",
    "        for split_type, source_fold_list in [(\"train\", current_train_fold_names), (\"val\", [current_val_fold_name])]:\n",
    "            print(f\"  Processing {split_type} data for CV Iteration {i+1}...\")\n",
    "            split_start_time = time.time()\n",
    "\n",
    "            coco_output_data = {\n",
    "                \"info\": {\n",
    "                    \"description\": f\"COCO-style dataset for CV Iteration {i+1} - {split_type}\",\n",
    "                    \"version\": \"1.0\",\n",
    "                    \"year\": datetime.date.today().year,\n",
    "                    \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
    "                },\n",
    "                \"licenses\": [{\"name\": \"Placeholder License\", \"id\": 0, \"url\": \"\"}], # Add licenses if any\n",
    "                \"categories\": coco_categories,\n",
    "                \"images\": [],\n",
    "                \"annotations\": []\n",
    "            }\n",
    "            \n",
    "            current_image_id = 1  # Reset for each JSON file (train/val)\n",
    "            current_annotation_id = 1 # Reset for each JSON file\n",
    "\n",
    "            tasks_for_split = []\n",
    "            img_counter_for_naming = 0 # To ensure unique names if files from different folds have same name\n",
    "\n",
    "            target_image_dir_for_split = train_img_dir if split_type == \"train\" else val_img_dir\n",
    "\n",
    "            for fold_idx, fold_name in enumerate(source_fold_list):\n",
    "                source_fold_path = os.path.join(source_dir, fold_name)\n",
    "                if not os.path.isdir(source_fold_path):\n",
    "                    print(f\"    Warning: Source fold '{source_fold_path}' for {split_type} not found. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                image_basenames = sorted([os.path.splitext(f)[0] for f in os.listdir(source_fold_path) if f.lower().endswith(\".png\")])\n",
    "                for img_basename in image_basenames:\n",
    "                    # Create a potentially more unique filename for the destination to avoid clashes\n",
    "                    # if images from different source folds (now combined to train) have same names.\n",
    "                    new_img_filename = f\"{split_type}_fold{fold_idx}_{img_basename}.png\"\n",
    "                    tasks_for_split.append((img_basename, source_fold_path, new_img_filename))\n",
    "            \n",
    "            if not tasks_for_split:\n",
    "                print(f\"    No images found to process for {split_type} set in this CV iteration.\")\n",
    "            else:\n",
    "                processed_results = []\n",
    "                with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    future_to_task = {executor.submit(_process_single_image_to_coco_data, task_args): task_args for task_args in tasks_for_split}\n",
    "                    for future in concurrent.futures.as_completed(future_to_task):\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result:\n",
    "                                processed_results.append(result)\n",
    "                        except Exception as exc:\n",
    "                            # task_args_failed = future_to_task[future]\n",
    "                            # print(f\"    Warning (COCO Split Processor): Task for {task_args_failed[0]} generated an exception: {exc}\")\n",
    "                            pass\n",
    "                \n",
    "                # Now, sequentially build the COCO lists to ensure unique IDs\n",
    "                for result_data in processed_results:\n",
    "                    # Copy image file\n",
    "                    shutil.copy2(result_data[\"original_source_path\"], os.path.join(target_image_dir_for_split, result_data[\"image_info\"][\"file_name\"]))\n",
    "                    \n",
    "                    # Add image entry\n",
    "                    img_entry = result_data[\"image_info\"]\n",
    "                    img_entry[\"id\"] = current_image_id \n",
    "                    # Remove helper key before adding to COCO JSON\n",
    "                    del img_entry[\"original_path\"] \n",
    "                    coco_output_data[\"images\"].append(img_entry) \n",
    "                    \n",
    "                    # Add annotation entries\n",
    "                    for ann_data in result_data[\"annotations_data\"]:\n",
    "                        if ann_data[\"category_name\"] not in category_name_to_id:\n",
    "                            # This should ideally not happen if discovery was thorough\n",
    "                            print(f\"    Skipping annotation with unknown category: {ann_data['category_name']}\")\n",
    "                            continue\n",
    "                        \n",
    "                        ann_entry = {\n",
    "                            \"id\": current_annotation_id,\n",
    "                            \"image_id\": current_image_id,\n",
    "                            \"category_id\": category_name_to_id[ann_data[\"category_name\"]],\n",
    "                            \"bbox\": ann_data[\"bbox\"],\n",
    "                            \"area\": ann_data[\"area\"],\n",
    "                            \"iscrowd\": 0,\n",
    "                            \"segmentation\": [] # Add segmentation if you have it\n",
    "                        }\n",
    "                        coco_output_data[\"annotations\"].append(ann_entry)\n",
    "                        current_annotation_id += 1\n",
    "                    current_image_id += 1\n",
    "            \n",
    "            # Write the COCO JSON file for the current split (train or val)\n",
    "            output_json_filename = f\"instances_{split_type}.json\" # e.g. instances_train.json\n",
    "            output_json_path = os.path.join(annotations_dir, output_json_filename)\n",
    "            \n",
    "            try:\n",
    "                with open(output_json_path, 'w') as f:\n",
    "                    json.dump(coco_output_data, f, indent=4) # indent for readability\n",
    "                split_processing_time = time.time() - split_start_time\n",
    "                print(f\"    Successfully created '{output_json_filename}' with {len(coco_output_data['images'])} images and {len(coco_output_data['annotations'])} annotations in {split_processing_time:.2f}s.\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error writing COCO JSON '{output_json_filename}': {e}\")\n",
    "        \n",
    "        cv_iteration_time = time.time() - cv_iteration_start_time\n",
    "        print(f\"  CV Iteration {i+1} processing took {cv_iteration_time:.2f}s.\")\n",
    "\n",
    "    total_script_time = time.time() - overall_start_time\n",
    "    print(f\"\\n{num_cv_folds}-Fold Cross-Validation COCO dataset preparation complete in {total_script_time:.2f} seconds!\")\n",
    "    print(f\"All CV iteration datasets are ready under: {base_dest_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97f8fcd-33df-454e-9f87-a37fe7317672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting COCO dataset creation for Co-DETR with K-Fold CV:\n",
      "  Source Directory: /blue/hulcr/gmarais/PhD/phase_1_data/1_data_splitting/classification_folds_output\n",
      "  Base Destination Directory: /blue/hulcr/gmarais/PhD/phase_1_data/3_classification_phase_2/coco\n",
      "  Source Fold Names: ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']\n",
      "  Max Workers for Parallelization: 6\n",
      "------------------------------\n",
      "Discovering COCO categories across all specified folds (parallelized)...\n",
      "Discovered 63 COCO categories in 1.57 seconds:\n",
      "  ID: 1, Name: Ambrosiodmus_minor\n",
      "  ID: 2, Name: Ambrosiophilus_atratus\n",
      "  ID: 3, Name: Anisandrus_dispar\n",
      "  ID: 4, Name: Anisandrus_sayi\n",
      "  ID: 5, Name: Cnestus_mutilatus\n",
      "  ID: 6, Name: Coccotrypes_carpophagus\n",
      "  ID: 7, Name: Coccotrypes_dactyliperda\n",
      "  ID: 8, Name: Coptoborus_ricini\n",
      "  ID: 9, Name: Cryptocarenus_heveae\n",
      "  ID: 10, Name: Ctonoxylon_hagedorn\n",
      "  ID: 11, Name: Cyclorhipidion_pelliculosum\n",
      "  ID: 12, Name: Dendroctonus_rufipennis\n",
      "  ID: 13, Name: Dendroctonus_terebrans\n",
      "  ID: 14, Name: Dendroctonus_valens\n",
      "  ID: 15, Name: Dryocoetes_autographus\n",
      "  ID: 16, Name: Euplatypus_compositus\n",
      "  ID: 17, Name: Euwallacea_fornicatus\n",
      "  ID: 18, Name: Euwallacea_perbrevis\n",
      "  ID: 19, Name: Euwallacea_validus\n",
      "  ID: 20, Name: Hylastes_porculus\n",
      "  ID: 21, Name: Hylastes_salebrosus\n",
      "  ID: 22, Name: Hylesinus_aculeatus\n",
      "  ID: 23, Name: Hylesinus_crenatus\n",
      "  ID: 24, Name: Hylesinus_toranio\n",
      "  ID: 25, Name: Hylesinus_varius\n",
      "  ID: 26, Name: Hylurgops_palliatus\n",
      "  ID: 27, Name: Hylurgus_ligniperda\n",
      "  ID: 28, Name: Hypothenemus_hampei\n",
      "  ID: 29, Name: Ips_acuminatus\n",
      "  ID: 30, Name: Ips_avulsus\n",
      "  ID: 31, Name: Ips_calligraphus\n",
      "  ID: 32, Name: Ips_duplicatus\n",
      "  ID: 33, Name: Ips_grandicollis\n",
      "  ID: 34, Name: Ips_sexdentatus\n",
      "  ID: 35, Name: Ips_typographus\n",
      "  ID: 36, Name: Monarthrum_fasciatum\n",
      "  ID: 37, Name: Monarthrum_mali\n",
      "  ID: 38, Name: Myoplatypus_flavicornis\n",
      "  ID: 39, Name: Orthotomicus_caelatus\n",
      "  ID: 40, Name: Orthotomicus_erosus\n",
      "  ID: 41, Name: Pagiocerus_frontalis\n",
      "  ID: 42, Name: Phloeosinus_dentatus\n",
      "  ID: 43, Name: Pityogenes_chalcographus\n",
      "  ID: 44, Name: Pityophthorus_juglandis\n",
      "  ID: 45, Name: Platypus_cylindrus\n",
      "  ID: 46, Name: Platypus_koryoensis\n",
      "  ID: 47, Name: Pycnarthrum_hispidium\n",
      "  ID: 48, Name: Scolytodes_glaber\n",
      "  ID: 49, Name: Scolytus_multistriatus\n",
      "  ID: 50, Name: Scolytus_schevyrewi\n",
      "  ID: 51, Name: Taphrorychus_bicolor\n",
      "  ID: 52, Name: Tomicus_destruens\n",
      "  ID: 53, Name: Trypodendron_domesticum\n",
      "  ID: 54, Name: Xyleborinus_saxesenii\n",
      "  ID: 55, Name: Xyleborus_affinis\n",
      "  ID: 56, Name: Xyleborus_celsus\n",
      "  ID: 57, Name: Xyleborus_ferrugineus\n",
      "  ID: 58, Name: Xyleborus_glabratus\n",
      "  ID: 59, Name: Xylosandrus_amputatus\n",
      "  ID: 60, Name: Xylosandrus_compactus\n",
      "  ID: 61, Name: Xylosandrus_crassiusculus\n",
      "  ID: 62, Name: Xylosandrus_germanus\n",
      "  ID: 63, Name: Xylosandrus_morigerus\n",
      "\n",
      "Preparing data for 5-Fold Cross-Validation (COCO Format)...\n",
      "\n",
      "--- Processing CV Iteration 1/5 ---\n",
      "  Output to: /blue/hulcr/gmarais/PhD/phase_1_data/3_classification_phase_2/coco/cv_iteration_1\n",
      "  Processing train data for CV Iteration 1...\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for Your Dataset (COCO Format) ---\n",
    "\n",
    "# **IMPORTANT**: Modify these paths and names to match your actual dataset and desired output.\n",
    "SOURCE_DIR = \"/blue/hulcr/gmarais/PhD/phase_1_data/1_data_splitting/classification_folds_output\"  # CHANGE THIS\n",
    "BASE_DEST_DIR = \"/blue/hulcr/gmarais/PhD/phase_1_data/3_classification_phase_2/coco\" # CHANGE THIS\n",
    "SOURCE_FOLD_NAMES_STR = \"fold1,fold2,fold3,fold4,fold5\" # CHANGE THIS\n",
    "\n",
    "# Optional: Control the number of parallel processes\n",
    "import os # if you want to use os.cpu_count() explicitly\n",
    "MAX_WORKERS = 6 # Defaults to os.cpu_count()\n",
    "# MAX_WORKERS = 4 # Or set a specific number\n",
    "\n",
    "# --- Run the Dataset Creation ---\n",
    "print(f\"Starting COCO dataset creation for Co-DETR with K-Fold CV:\")\n",
    "print(f\"  Source Directory: {SOURCE_DIR}\")\n",
    "print(f\"  Base Destination Directory: {BASE_DEST_DIR}\")\n",
    "print(f\"  Source Fold Names: {SOURCE_FOLD_NAMES_STR.split(',')}\")\n",
    "print(f\"  Max Workers for Parallelization: {MAX_WORKERS if MAX_WORKERS is not None else f'Default (likely {os.cpu_count()})'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Ensure the function create_kfold_coco_datasets is defined by running the cell above first!\n",
    "create_kfold_coco_datasets(\n",
    "    source_dir=SOURCE_DIR,\n",
    "    base_dest_dir=BASE_DEST_DIR,\n",
    "    source_fold_names_str=SOURCE_FOLD_NAMES_STR,\n",
    "    max_workers=MAX_WORKERS\n",
    ")\n",
    "\n",
    "print(\"\\nCOCO Script execution finished in this cell.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEL",
   "language": "EEL",
   "name": "eel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
